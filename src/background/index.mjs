import Browser from 'webextension-polyfill'
import {
  deleteConversation,
  generateAnswersWithChatgptWebApi,
  sendMessageFeedback,
} from '../services/apis/chatgpt-web'
import { generateAnswersWithBingWebApi } from '../services/apis/bing-web.mjs'
import {
  generateAnswersWithChatgptApi,
  generateAnswersWithGptCompletionApi,
} from '../services/apis/openai-api'
import { generateAnswersWithCustomApi } from '../services/apis/custom-api.mjs'
import { generateAnswersWithAzureOpenaiApi } from '../services/apis/azure-openai-api.mjs'
import { generateAnswersWithClaudeApi } from '../services/apis/claude-api.mjs'
import { generateAnswersWithChatGLMApi } from '../services/apis/chatglm-api.mjs'
import { generateAnswersWithWaylaidwandererApi } from '../services/apis/waylaidwanderer-api.mjs'
import {
  azureOpenAiApiModelKeys,
  claudeApiModelKeys,
  chatglmApiModelKeys,
  bardWebModelKeys,
  bingWebModelKeys,
  chatgptApiModelKeys,
  chatgpt4vApiModelKeys,
  chatgptWebModelKeys,
  claudeWebModelKeys,
  customApiModelKeys,
  defaultConfig,
  getUserConfig,
  githubThirdPartyApiModelKeys,
  gptApiModelKeys,
  poeWebModelKeys,
  setUserConfig,
} from '../config/index.mjs'
import '../_locales/i18n'

import { TokenTextSplitter } from 'langchain/text_splitter'
import { MemoryVectorStore } from 'langchain/vectorstores/memory'
import { OpenAIEmbeddings } from '@langchain/openai'
import { Document } from 'langchain/document'

import { openUrl } from '../utils/open-url'
import {
  getBardCookies,
  getBingAccessToken,
  getChatGptAccessToken,
  getClaudeSessionKey,
  registerPortListener,
} from '../services/wrappers.mjs'
import { refreshMenu } from './menus.mjs'
import { registerCommands } from './commands.mjs'
import { generateAnswersWithBardWebApi } from '../services/apis/bard-web.mjs'
import { generateAnswersWithClaudeWebApi } from '../services/apis/claude-web.mjs'
import { generateAnswersWithChatgpt4vApi } from '../services/apis/openai-api.mjs'
import { getCapturedBase64Screenshot, setCapturedBase64Screenshot } from './screenshot.mjs'

const printApiInfo = true

function setPortProxy(port, proxyTabId) {
  port.proxy = Browser.tabs.connect(proxyTabId)
  const proxyOnMessage = (msg) => {
    port.postMessage(msg)
  }
  const portOnMessage = (msg) => {
    port.proxy.postMessage(msg)
  }
  const proxyOnDisconnect = () => {
    port.proxy = Browser.tabs.connect(proxyTabId)
  }
  const portOnDisconnect = () => {
    port.proxy.onMessage.removeListener(proxyOnMessage)
    port.onMessage.removeListener(portOnMessage)
    port.proxy.onDisconnect.removeListener(proxyOnDisconnect)
    port.onDisconnect.removeListener(portOnDisconnect)
  }
  port.proxy.onMessage.addListener(proxyOnMessage)
  port.onMessage.addListener(portOnMessage)
  port.proxy.onDisconnect.addListener(proxyOnDisconnect)
  port.onDisconnect.addListener(portOnDisconnect)
}

async function executeApi(session, port, config) {
  console.debug('modelName', session.modelName)
  if (chatgptWebModelKeys.includes(session.modelName)) {
    let tabId
    if (
      config.chatgptTabId &&
      config.customChatGptWebApiUrl === defaultConfig.customChatGptWebApiUrl
    ) {
      const tab = await Browser.tabs.get(config.chatgptTabId).catch(() => {})
      if (tab) tabId = tab.id
    }
    if (tabId) {
      if (!port.proxy) {
        setPortProxy(port, tabId)
        port.proxy.postMessage({ session })
      }
    } else {
      const accessToken = await getChatGptAccessToken()
      await generateAnswersWithChatgptWebApi(port, session.question, session, accessToken)
    }
  } else if (
    // `.some` for multi mode models. e.g. bingFree4-balanced
    bingWebModelKeys.some((n) => session.modelName.includes(n))
  ) {
    const accessToken = await getBingAccessToken()
    if (session.modelName.includes('bingFreeSydney'))
      await generateAnswersWithBingWebApi(port, session.question, session, accessToken, true)
    else await generateAnswersWithBingWebApi(port, session.question, session, accessToken)
  } else if (gptApiModelKeys.includes(session.modelName)) {
    await generateAnswersWithGptCompletionApi(
      port,
      session.question,
      session,
      config.apiKey,
      session.modelName,
    )
  } else if (chatgptApiModelKeys.includes(session.modelName)) {
    await generateAnswersWithChatgptApi(
      port,
      session.question,
      session,
      config.apiKey,
      session.modelName,
    )
  } else if (chatgpt4vApiModelKeys.includes(session.modelName)) {
    // This is the part for image based processing
    // if (session.image_url === '') throw new Error("GPT4v model selected, but no image input, select another model or use the option that can create image.")
    if (printApiInfo) {
      console.log(chatgpt4vApiModelKeys)
      console.log('gpt4v section called')
    }
    // const tabIdToShot = getBrowserIdToShot();
    // console.log("tab id used: ", tabIdToShot)
    // const testima = await Browser.tabs.captureVisibleTab(null, { format: 'png' })
    // console.log(testima)
    if (!('image_url' in session) || session.image_url === '') {
      console.log('testing')
      const res = getCapturedBase64Screenshot()
      console.log('initial image url: ', session.image_url)
      session.image_url = res
      console.log('updated image url: ', res)
    } else {
      console.log('image already set: ', session.image_url)
      console.log('globally saved image: ', getCapturedBase64Screenshot())
    }

    // chrome.tabs.captureVisibleTab(tabIdToShot, { format: 'png' }, (screenshotDataUrl) => {
    // // chrome.tabs.captureVisibleTab((screenshotDataUrl) => {

    //   // const screenshotImage = new Image();
    //   // screenshotImage.src = screenshotDataUrl;
    //   console.log("screenshot url: ")
    //   console.log(screenshotDataUrl)
    //   window.open(screenshotDataUrl)
    // });
    await generateAnswersWithChatgpt4vApi(
      port,
      session.question,
      session,
      config.apiKey,
      session.modelName,
    )
  } else if (customApiModelKeys.includes(session.modelName)) {
    await generateAnswersWithCustomApi(
      port,
      session.question,
      session,
      config.customApiKey,
      config.customModelName,
    )
  } else if (azureOpenAiApiModelKeys.includes(session.modelName)) {
    await generateAnswersWithAzureOpenaiApi(port, session.question, session)
  } else if (claudeApiModelKeys.includes(session.modelName)) {
    await generateAnswersWithClaudeApi(port, session.question, session)
  } else if (chatglmApiModelKeys.includes(session.modelName)) {
    await generateAnswersWithChatGLMApi(port, session.question, session, session.modelName)
  } else if (githubThirdPartyApiModelKeys.includes(session.modelName)) {
    await generateAnswersWithWaylaidwandererApi(port, session.question, session)
  } else if (poeWebModelKeys.includes(session.modelName)) {
    throw new Error('Due to the new verification, Poe Web API is currently not supported.')
    // if (session.modelName === 'poeAiWebCustom')
    //   await generateAnswersWithPoeWebApi(port, session.question, session, config.poeCustomBotName)
    // else
    //   await generateAnswersWithPoeWebApi(
    //     port,
    //     session.question,
    //     session,
    //     Models[session.modelName].value,
    //   )
  } else if (bardWebModelKeys.includes(session.modelName)) {
    const cookies = await getBardCookies()
    await generateAnswersWithBardWebApi(port, session.question, session, cookies)
  } else if (claudeWebModelKeys.includes(session.modelName)) {
    const sessionKey = await getClaudeSessionKey()
    await generateAnswersWithClaudeWebApi(
      port,
      session.question,
      session,
      sessionKey,
      session.modelName,
    )
  }
}

Browser.runtime.onMessage.addListener(async (message, sender) => {
  switch (message.type) {
    case 'FEEDBACK': {
      const token = await getChatGptAccessToken()
      await sendMessageFeedback(token, message.data)
      break
    }
    case 'DELETE_CONVERSATION': {
      const token = await getChatGptAccessToken()
      await deleteConversation(token, message.data.conversationId)
      break
    }
    case 'NEW_URL': {
      const newTab = await Browser.tabs.create({
        url: message.data.url,
        pinned: message.data.pinned,
      })
      if (message.data.saveAsChatgptConfig) {
        await setUserConfig({
          chatgptTabId: newTab.id,
          chatgptJumpBackTabId: sender.tab.id,
        })
      }
      break
    }
    case 'SET_CHATGPT_TAB': {
      await setUserConfig({
        chatgptTabId: sender.tab.id,
      })
      break
    }
    case 'ACTIVATE_URL':
      await Browser.tabs.update(message.data.tabId, { active: true })
      break
    case 'OPEN_URL':
      openUrl(message.data.url)
      break
    case 'OPEN_CHAT_WINDOW': {
      const config = await getUserConfig()
      const url = Browser.runtime.getURL('IndependentPanel.html')
      const tabs = await Browser.tabs.query({ url: url, windowType: 'popup' })
      if (!config.alwaysCreateNewConversationWindow && tabs.length > 0)
        await Browser.windows.update(tabs[0].windowId, { focused: true })
      else
        await Browser.windows.create({
          url: url,
          type: 'popup',
          width: 500,
          height: 650,
        })
      break
    }
    case 'REFRESH_MENU':
      refreshMenu()
      break
    case 'PIN_TAB': {
      let tabId
      if (message.data.tabId) tabId = message.data.tabId
      else tabId = sender.tab.id

      await Browser.tabs.update(tabId, { pinned: true })
      if (message.data.saveAsChatgptConfig) {
        await setUserConfig({ chatgptTabId: tabId })
      }
      break
    }
    case 'FETCH': {
      if (message.data.input.includes('bing.com')) {
        const accessToken = await getBingAccessToken()
        await setUserConfig({ bingAccessToken: accessToken })
      }

      try {
        const response = await fetch(message.data.input, message.data.init)
        const text = await response.text()
        return [
          {
            body: text,
            status: response.status,
            statusText: response.statusText,
            headers: Object.fromEntries(response.headers),
          },
          null,
        ]
      } catch (error) {
        return [null, error]
      }
    }
  }
})

try {
  Browser.webRequest.onBeforeRequest.addListener(
    (details) => {
      if (
        details.url.includes('/public_key') &&
        !details.url.includes(defaultConfig.chatgptArkoseReqParams)
      ) {
        let formData = new URLSearchParams()
        for (const k in details.requestBody.formData) {
          formData.append(k, details.requestBody.formData[k])
        }
        setUserConfig({
          chatgptArkoseReqUrl: details.url,
          chatgptArkoseReqForm:
            formData.toString() ||
            new TextDecoder('utf-8').decode(new Uint8Array(details.requestBody.raw[0].bytes)),
        }).then(() => {
          console.log('Arkose req url and form saved')
        })
      }
    },
    {
      urls: ['https://*.openai.com/*'],
      types: ['xmlhttprequest'],
    },
    ['requestBody'],
  )

  Browser.webRequest.onBeforeSendHeaders.addListener(
    (details) => {
      const headers = details.requestHeaders
      for (let i = 0; i < headers.length; i++) {
        if (headers[i].name === 'Origin') {
          headers[i].value = 'https://www.bing.com'
        } else if (headers[i].name === 'Referer') {
          headers[i].value = 'https://www.bing.com/search?q=Bing+AI&showconv=1&FORM=hpcodx'
        }
      }
      return { requestHeaders: headers }
    },
    {
      urls: ['wss://sydney.bing.com/*', 'https://www.bing.com/*'],
      types: ['xmlhttprequest', 'websocket'],
    },
    ['requestHeaders'],
  )

  Browser.tabs.onUpdated.addListener(async (tabId, info, tab) => {
    if (!tab.url) return
    // eslint-disable-next-line no-undef
    await chrome.sidePanel.setOptions({
      tabId,
      path: 'IndependentPanel.html',
      enabled: true,
    })
  })
} catch (error) {
  console.log(error)
}

function registerImageSaver() {
  Browser.runtime.onMessage.addListener(async (message) => {
    console.log('image_saver called ')
    console.log(message)
    if (message.type === 'SAVE_IMAGE') {
      setCapturedBase64Screenshot(message.imageUrl)
    }
  })
}

function recordDocument(documentName, documentContent) {
  const splitter = new TokenTextSplitter({
    encodingName: 'gpt2',
    chunkSize: 100,
    chunkOverlap: 0,
  })
  splitter.createDocuments([documentContent]).then(async (res) => {
    console.log(res)
    const embeddings = new OpenAIEmbeddings({
      openAIApiKey: '', // In Node.js defaults to process.env.OPENAI_API_KEY
      batchSize: 512, // Default value if omitted is 512. Max is 2048
      modelName: 'text-embedding-3-small',
    })

    const docStringList = res.map((doc) => {
      return doc.pageContent
    })
    const docEmbedding = await embeddings.embedDocuments(docStringList)

    const embeddingsToSave = {}
    embeddingsToSave[documentName] = {
      vector: docEmbedding,
      text: docStringList,
    }
    let docNameList = await Browser.storage.local.get('docNameList')

    // adding the indexing
    if (Object.keys(docNameList).length === 0) {
      const docNameList = []
      docNameList.push(documentName)
      await Browser.storage.local.set({ docNameList: docNameList })
    } else {
      docNameList = docNameList['docNameList']
      docNameList.push(documentName)
      await Browser.storage.local.set({ docNameList: docNameList })
    }
    // set the data
    await Browser.storage.local.set(embeddingsToSave)
    console.log(docNameList)

    console.log(await Browser.storage.local.get(documentName))
  })
}

async function checkSaveInfoIntegrity() {
  // Do a check to see if all recorded files in docNameList exists in the local storage
  let fileNameList = await Browser.storage.local.get('docNameList')
  if (Object.keys(fileNameList).length === 0) return
}

async function registerTextSpliter() {
  console.log('started')
  const documentContent ="(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 220Multimodal Learning Analytics and Education Data Mining: UsingComputational Technologies to Measure Complex Learning TasksPaulo BliksteinGraduate School of Education and (by courtesy) Computer Science DepartmentStanford University, USApaulob@stanford.eduMarcelo WorsleyLearning Sciences & Computer ScienceNorthwestern University, USAmarcelo.worsley@northwestern.eduABSTRACT: New high-frequency multimodal data collection technologies and machine learninganalysis techniques could offer new insights into learning, especially when students have theopportunity to generate unique, personalized artifacts, such as computer programs, robots, andsolutions engineering challenges. To date most of the work on learning analytics and educationaldata mining has been focused on online courses and cognitive tutors, both of which provide ahigh degree of structure to the tasks, and are restricted to interactions that occur in front of acomputer screen. In this paper, we argue that multimodal learning analytics can offer newinsights into student learning trajectories in more complex and open-ended learningenvironments. We present several examples of this work and its educational applications.Keywords: Learning analytics, multimodal interaction, constructivism, constructionism,assessment1 INTRODUCTIONThe same battle is fought in every field of educational research and practice: the champions of the directinstruction of well-defined content pitted against those who encourage student-centred exploration ofill-defined domains. These wars have taken place repeatedly over past decades, and partisans on eachside have been reborn in multiple incarnations. The first tradition tends to be aligned with behaviouristor neo-behaviourist approaches, while the second favours constructivist-inspired pedagogies. Inlanguage arts, the battle has been between phonics and the whole word approach. In math, war iswagged between teaching algorithms versus instruction in how to think mathematically. In history, theyclash over the relative merits of critical interpretations and the memorization of historical facts. Inscience, they clash about inquiry-based approaches versus direct instruction of formulas and principles.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 221The educational research community has always maintained that the debate would end when researchresults inevitably demonstrated the superiority of one of the sides. Yet this conclusion has eludedscholarship for decades. One of the reasons for this interminable contest is that the underlying rationalefor the differences concerns individual values and societal beliefs and will not be resolved by a purelyscientific approach. In fact, the whole debate may serve the educational research community in quite adifferent capacity. More specifically the debates may reveal the underlying visions for what educationshould be about, for different groups, and we might more profitably re-examine the nature and purposeof our schools. More fundamentally, is education a tool for filtering, ranking, emancipation, socialequalization, economic progress, meritocracy, or for the promotion of social Darwinism?Educational scholars would greatly differ in their answers — rendering the question of “which approachis better,” and “what evidence suffices,” pointless. As with the debates on public healthcare and fiscalpolicy, despite our best efforts to generate reliable research, the “best” way to conduct education willalways be controversial and dependent on larger societal and political winds. But the fundamentalproblem, and the motivation for this article, is that the prevailing issue is not who “wins” the debate, butrather the existence of a healthy debate. Fostering a healthy debate requires some level of symmetry.However, as it stands, the playing field is not symmetrical. The “direct instruction” approach isinherently easier to test and quantify using currently available tools that include mass-production ofcontent and decades of research concerning psychometrics and standardized testing strategies.Meanwhile, the constructivist side counts on laborious interventions, and complex mixed-moderesearch methods. The result of this asymmetry is that public systems, more dependent on high-profileresearch results, are left, by inertia, to the designs of the proponents of traditional approaches, whileonly affluent schools, private or public, who can experiment more, can afford to implement modern,constructivist approaches to learning.Learning analytics could deepen this asymmetry, or help eliminate it. The elimination of the asymmetrycould re-establish a healthy public debate around education, where both sides would have comparableand credible results to show, and policy makers would be able to make choices based on their valuesand visions for education. However, the deepening of this asymmetry could be a significant impedimentto progressive education and the vision of creating alternative learning environments that can reach amore diverse population of learners. Should public education succumb to the temptation of the fiscalbenefits supposedly offered by total automatization and its much lower baseline for cost and quality, allother options would be driven into the ground as economically unfeasible: who could compete withvirtually free computerized tutors and videos? How many years would the debate take, while childrencaught in the “experimental” years are being victimized?Consequently, we propose that an important goal of learning analytics is to equalize the playing field bydeveloping methods that examine and quantify non-standardized forms of learning. We suggest thatthis need for a level playing field is more necessary than ever, given the increasing demand for scalableproject-based, interest-driven learning and student-centred pedagogies (e.g., Papert, 1980). Within our(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 222increasingly interconnected societal and economic environment — which has become pervaded bytechnology and threatened by challenging global problems such as climate change — both K–12 anduniversity-level engineering education (Dym, 1999) demand higher-level, complex problem-solving asopposed to performance in routine cognitive tasks (Levy & Murnane, 2004). Approaches that placepremiums on student-centred, constructivist, self-motivated, self-directed learning have beenadvocated for decades (e.g., Dewey, 1902; Freire, 1970; Montessori, 1965; Barron & Darling-Hammond,2010) but have failed to become scalable and prevalent, and have come under attack during the lastdecade (e.g., Kirschner, Sweller, & Clark, 2006; Klahr & Nigam, 2004).New high-frequency data collection technologies and machine learning could offer new insights intolearning in tasks in which students are allowed to generate unique, personalized artifacts, such ascomputer programs, robots, movies, animations, and solutions to engineering challenges. To date mostof the work on learning analytics and educational data mining has focused on tasks that are computer-mediated and are more structured and scripted. In this work, we argue that multimodal data collectionand analysis techniques (“multimodal learning analytics” or MMLA) could yield novel methods thatgenerate distinctive insights into what happens when students create unique solution paths toproblems, interact with peers, and act in both the physical and digital worlds.Assessment and feedback is particularly difficult within these open-ended environments, and theselimitations have hampered many attempts to make such approaches more prevalent. Automated, fine-grained data collection and analysis could help resolve this tension in two ways. First, such capacitieswould give researchers tools to examine student-centred learning in unprecedented scale and detail.Second, these techniques could improve the scalability of these pedagogies since they make feasibleboth assessment and formative feedback, which are typically very complex and laborious in suchenvironments. They might not only reveal students’ trajectories throughout specific learning activities,but they could also help researchers design better supports, pedagogical approaches, and learningmaterials.At the same time, in the well-established field of multimodal interaction, new data collection andsensing technologies are making it possible to capture massive amounts of data in all fields of humanactivity. These technologies include the logging of computer activities, wearable cameras, wearablesensors, biosensors (e.g., that permit measurements of skin conductivity, heartbeat, andelectroencephalography), gesture sensing, infrared imaging, and eye tracking. Such techniques enableresearchers to have unprecedented insight into the minute-by-minute development of a number ofactivities, especially those involving multiple dimensions of activity and social interaction. However, thetechnologies just mentioned have not yet become popular in the field of learning analytics. We proposethat multimodal learning analytics could bring together these multiple techniques in morecomprehensive evaluations of complex cognitive abilities, especially in environments where theprocesses or outcomes are unscripted. Thus, the goal of this paper is to demonstrate the feasibility andpower of novel assessment techniques in several modalities and learning contexts.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 2232 STATE OF THE FIELDIn considering the current sensing and assessment modalities possible using MMLA, we see three non-mutually exclusive areas: assessing student knowledge, assessing student affect and physiology, andassessing student intentions or beliefs. At the crux of all these forms of student characterization is theunderlying invocation of data analysis to generate useful models from large sets of quantitative data.Hence, what varies in the different forms of student assessment is the source of the raw data and howthat data is translated into computable data. Once the translation has been completed, the data isprocessed using a collection of machine learning algorithms. In what follows, we present severalmethods being used to capture and process student data. There are several techniques — web datamining, user data mining, simple web-based surveys, etc. — but the following technologies have beenselected for inclusion because they live on the cutting-edge of technology and help promote the notionof “natural” assessment (Zaïane, 2001). Furthermore, while each of these technologies represents aresearch contribution in and of itself, our interest in including them is to bring to the forefront a widervariety of non-traditional approaches that education researchers and educational data scientists canbegin to combine in their learning analytics research. For the first three techniques we mention — textanalysis, speech analysis, and handwriting analysis — our discussion will be very cursory, as theserepresent areas of research that have received considerable attention with the computer sciencecommunity, and have started to get traction within the learning analytics community. Nonetheless, wewant to make the reader aware of some of the current capabilities and research in these areas. For thelatter analyses that we discuss, we will engage in a more detailed and descriptive explanation of each, asthese domains remain relatively new, even among the computer science community.2.1 Text AnalysisWhile text analysis, or natural language processing, has been around for decades it is only in recenthistory that education has begun to benefit from this technology, and researchers have targetedlearners’ text explicitly. Despite the fact that text itself is not multimodal, text analysis allows for theinterpretation of open-ended writing tasks, differently from multiple-choice tests. Given that collectingtext from students is unproblematic both technically and logistically, it constitutes one of the mostpromising modalities for MMLA: text can be easily gathered from face-to-face and online activities, fromtests and exams, and from expert-generated prose from textbooks and online sources (often used asbaseline). For example, Sherin (2013) has been doing pioneering work in the analysis of text in thelearning sciences community. He uses techniques from topic modelling and clustering to study theprogression of students’ ideas and intuitions as they describe the explanation for the existence of thefour seasons (Sherin, 2013). More specifically, he shows that, as students explain the seasons, invokingdifferent types of scientific explanations, it is possible to identify which type of explanation each studentis referring to at different points in time. He also goes beyond this to show how students can beaccurately clustered without using a pre-defined set of exemplar responses, but instead by usingautomatically derived topics models from the corpus itself. This approach of clustering segments of(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 224students’ text based on the descriptions of their peers is a powerful tool that can allow researchers andpractitioners to draw meaningful commonalities and differences among large populations of students,without having to explicitly read and compare the entirety of each transcript. Given the prevalence oftext-based assessment and the intensive use of text in face-to-face and online learning, this promisingmethod will likely accelerate discourse-based research, and open new possibilities for large-scaleanalysis of open-ended text corpora.2.2 Speech AnalysisSpeech analysis shares many of same goals and tools as text analysis. Speech analysis, however, furtherremoves the student from the traditional assessment setting by allowing them to demonstrate fluencyin a more natural setting. For example, Worsley & Blikstein (2011) studied how elements of studentspeech, as inferred by linguistic, textual, and prosodic features, can be predictive for identifyingstudents’ level of expertise on open-ended engineering design tasks. In addition to traditional linguisticand prosodic features, speech signals can be analyzed for a wealth of other characteristics. Variousresearch tools have been developed to help researchers in the process of extracting these features,however, several challenges remain in knowing how to analyze student learning appropriately using saidfeatures.Other researchers have moved away from raw analysis of the speech signal to leverage speechrecognition capabilities. In particular, Beck and Sison (2006) demonstrated a method for using speechrecognition to assess reading proficiency. As an extension of Project LISTEN — an intelligent tutor thathelps elementary school students improve their reading skills — researchers completed a study thatcombines speech recognition with knowledge tracing, a form of probabilistic monitoring. By having alanguage model that was largely restricted to the content of each book being learned, the work requiredfor doing automatic speech recognition, and subsequent accuracy classification, was greatly simplified.Outside of the education domain there have been decades of work in developing speech recognizersand dialogue managers. However, to date, such technologies are still not widely used in educationbecause of the challenges associated with building a satisfactory language model that can reliablyrecognize speech. Munteanu, Peng, and Zhu (2009) have made some progress in this area by showinghow to improve speech recognition of lectures in college-level STEM class. A primary consideration inthe area of speech recognition, therefore, will be to identify the most effective ways to use thistechnology in real-world educational settings. Although using it to transcribe lectures might be feasible,the challenge of collecting and interpreting student data seems extremely difficult. Differently fromother applications of speech recognition (smartphones, personal assistants, dictation), educationalapplications need to address simultaneously classroom noise, multiple overlapping speakers, andlogistical difficulties in voice training — very ambitious challenges that have not been solved yet.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 2252.3 Handwriting AnalysisA different form of text analysis is handwriting analysis, which is important in educational settingsbecause a considerable part of the work done by students is still handwritten. Anthony, Yang, andKoedinger (2007) highlight the affordances of combining handwriting recognition with intelligent tutorsfor algebra. Based on their study of high school and middle school students, introducing handwritingrecognition halved the time students needed to complete tutoring activities because students no longerhad to deal with cumbersome keyboard and mouse-based entry. This is significant because it enablesstudents to focus on understanding the material using familiar forms of interaction as opposed tostruggling to learn a new interface. Accordingly, handwriting recognition can facilitate more effectivelearning by eliminating the barriers to using certain computer-based interfaces. It also permits thestudent to learn in such a way that more closely parallels the usual mathematics environment (i.e.,utilizing a writing tool as opposed to a keyboard), which may increase transfer.Researchers also studied the use of handwriting recognition technology among school-aged children(Read, 2007), examining the length and quality of stories produced by students using different inputmethods. A primary finding of this work was that students were more willing to engage in the writingprocess when using digital ink than when using traditional keyboard input. However, the team still foundthat handwriting recognition technology was not yet comparable to traditional paper and pencil.Similarly to Anthony et al. (2007), Read (2007) emphasizes the affordances of handwriting as a morenatural form of authorship that may help students better engage in learning.More recent work extends handwriting recognition to mid-air “writing” that achieves high levels ofaccuracy by utilizing a combination of computer vision, multiple cameras, and machine learning (Schick,Morlock, Amma, Schultz, & Stiefelhagen, 2012). This approach highlights some of the more recentopportunities in handwriting recognition in novel learning environments and contributes to thediscussion around the expansive possibilities available away from traditional keyboards and screens.2.4 Sketch AnalysisWhereas handwriting analysis is primarily concerned with looking for words, others researchers haveembarked on work that looks at both text-based and graphic-based representations. Fundamental workon object recognition and sketches is that of Alvarado, Oltmans, and Davis (2002) and Alvarado andDavis (2006). These researchers developed a framework for performing multi-domain recognition ofsketches using Bayesian networks and a predetermined set of shapes and patterns for each domain.With the predefined shapes and patterns, their algorithm is able to decipher messy sketches from thedomains of interest.Ken Forbus and colleagues also describe seminal work in the development of both systems andtechniques for analyzing and comparing sketches among learners. For example, Jee, Gentner, Forbus,(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 226Sageman, & Uttal (2009) explain the design and implementation of CogSketch, a tool used to study howstudents of different levels of experience describe common scientific concepts in geology throughsketches (Forbus, Usher, Lovett, Lockwood, & Wetzel, 2011). CogSketch pays particular attention to boththe content and the process of the sketches being developed. Chang and Forbus (2012) extend this workon qualitative sketching to include quantitative analysis of sketching, which allows them to garner amore accurate representation and understanding of the sketches.Sketching is particularly important given the current focus on conceptual learning in STEM. One of themost popular forms of eliciting student knowledge in science has been the creation of diagrams andconcept maps. From this prior work, it is apparent that a number of research groups have demonstratedthe ability to do meaningful analyses of sketches in order to study cognition and learning.2.5 Action and Gesture AnalysisAction recognition has recently received considerable attention within the computer vision community.For example, work by Weinland, Ronfard, & Boyer (2006) and Yilmaz and Shah (2005), among others,has demonstrated the ability to detect basic human actions related to movement. The work of Weinlandet al. (2006) involved developing a technique that could capture user actions independent of gender,body size, and viewpoint. The work of Yilmaz and Shah (2005) involved human action recognition usinguncalibrated moving cameras, which might prove useful for the dynamic nature of classrooms and/orlaboratories.This kind of work is currently being applied to classroom settings as well. Raca, Tormey, and Dillenbourg(2014), for instance, are pioneering ways of capturing student engagement and attention by conductingframe-by-frame analyses of videos taken from the teacher’s position. They show that students’ motionand level of attention can be estimated using computer vision, and that individuals with lower levels ofattention are slower to react than focused students. This line of work opens the door for new kinds offeedback loops for teachers, by providing not only real-time information about students but alsoaggregate measures of their levels of attention over time.Other work in the area of gesture recognition has leveraged infrared cameras and accelerometers thatare affixed to the research subject. Using infrared, one avoids some of the complications that may existwith camera geometry, lighting, and other forms of visual variance. Using this approach Schlömer,Poppinga, Henze, and Boll (2008) demonstrate the ability to construct a gesture recognition system bycapturing and processing accelerometer data from a Nintendo Wiimote. Their technique allows them toreliably capture gestures for squares, circles, rolling, the shape “Z,” etc.More recent work has taken advantages of the Microsoft Kinect sensor and simple infrared detectors aslow cost tools for capturing and studying human gestures. The Mathematical Imagery Trainer (Howison,Trninic, Reinholz, & Abrahamson, 2011) uses hand gestures captured by the Kinect sensor as a way forstudying student understanding of proportions. Students use their hands to indicate the relationship(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 227between two values, and benefit from visual feedback on the correctness of their hand placement. Thissystem also enables teachers to give students real-time, immediate feedback and change theirinstruction as they perceive students’ difficulties (and not only after the fact), and points to one of themain benefits of multimodal learning analytics. As an even more basic example, the Kinect sensor can beused to give teachers and students immediate feedback about the amount of gesticulation that they aredoing. Hence, without requiring a set of recommended actions, low-cost sensing of movement could beuseful in helping students and teachers be more aware of their own behaviours.Related to the measurement of student gesticulation, early work in this domain by Worsley and Blikstein(2013) involved a comparison of hand/wrist movement between experts and novices as they completedan engineering design task. In particular, the researchers used hand/wrist movement data from a Kinectsensor to examine the extent of two-handed action, and found that experts were much more likely toemploy two-handed actions than novices. These preliminary results aligned with theories associatedwith two-handed inter-hemispheric actions, and provided initial motivation for studying gestures incomplex learning environments.In a similar line of work, Schneider and Blikstein (2014) used a Kinect sensor to evaluate studentstrategies when interacting with a Tangible User Interface (TUI): their task was to learn about the humanhearing system by interacting with 3D-printed organs of the inner ear. Using clustering algorithms, theauthors found that students’ body postures fell into three prototypical positions (Figure 1): an active,semi-active, or passive state. The amount of time spent in the active state was significantly correlatedwith higher learning gains, and the time spent in the passive state was significantly correlated with lowerlearning gains. More interestingly, the number of transitions between those states was the strongestpredictor of learning. Those results suggest that successful students went through cycles of reflectionand action, which helped them gain a deeper understanding of the domain taught. This approach showsthe potential of using clustering methods on gestures data to find recurring behaviours associated withhigher learning gain.Figure 1: Using k-means on student body posture (Schneider & Blikstein, 2014). The first state (left)is active, with both hands on the table; the second (middle) is passive, with arms crossed; the third(right) is semi-active, with only one hand on the table.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 228As a whole, the advances in action and gesture recognition, and the introduction of low-cost, high-accuracy sensors is creating additional opportunities for action and gesture recognition to be included ineducation research.2.6 Affective State AnalysisStudying students’ affective states can often be challenging and hard to validate. However, severalstudies have demonstrated that identifying affect can be done consistently, and that affect is animportant marker in studying and understanding learning.2.6.1 Human Annotated Affective StatesBaker, D’Mello, Rodrigo, & Graesser (2010) and Pardos, Baker, San Pedro, Gowda, & Gowda (2013) areexamples of work using human annotated affective states. In Pardos et al. (2013), the researchers usedthe Baker-Rodrigo Observation Method Protocol (BROMP) (Ocumpaugh, Baker, & Rodrigo, 2012) tocorrelate student behaviour and affect while participating in cognitive tutoring activities withperformance on standardized tests. They found that the learning gains associated with certain affectivestates, namely boredom and confusion, are highly dependent on the level of scaffolding that the studentis receiving. This finding builds on prior work that studies affective state as students participate incognitive tutoring activities (e.g., Litman, Moore, Dzikovska, & Farrow, 2009; Forbes-Riley, Rotaru, &Litman, 2009).2.6.2 Automatically Annotated Affective StateOther work, using the Facial Action Coding System (FACS), has demonstrated that researchers canrecognize student affective state by simply observing their facial expressions. In the case of Craig,D’Mello, Witherspoon, and Graesser (2008), researchers were able to perceive boredom, stress, andconfusion by applying machine learning to video data of the student’s face throughout the tutoringexperience. Data was collected while students interacted with AutoTutor, an intelligent tutoring systemfor learning science. The technique that Craig et al. (2008) validated is a highly non-invasive mechanismfor realizing student sentiment, and can be coupled with computer vision technology to enablemachines to detect changes in emotional state or cognitive-affect automatically. Worsley and Blikstein(2015) utilize the Facial Action Coding System to compare two different experimental conditions. Morespecifically, the authors compared the frequency and rate of transitions among four automaticallyderived affective states that are conjectured to be important for learning. In particular, they were ableto show that the two experimental conditions expressed significantly different rates of confusion anddiffered in how frequently they transitioned from neutral to surprise, and from neutral to confusion.Being in, or transitioning to a confused expression was generally associated with good outcomes,whereas being more surprised, or transitioning to an expression of surprise was generally associatedwith less favourable outcomes.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 229Researchers have also used conversational cues to realize students’ emotional states. Similar to Craig etal. (2008), D’Mello, Craig, Witherspoon, McDaniel, & Graesser (2008) designed an application that coulduse spoken dialogue to recognize the states of boredom, frustration, flow, and confusion. Researcherswere able to resolve the validity of their findings through comparison to emote-aloud (a derivative oftalk-aloud where participants describe their emotions as they feel them) activities while studentsinteracted with AutoTutor.2.6.3 Physiological Markers of Affective StateMore recent work in this space was able to accurately predict the affective state, and the source of thechange in affective state for users as they interact with a computer-based tutoring system (Conati &MacLaren, 2009). In particular, the system was able to predict when students experienced joy, distress,and admiration effectively. In the past years, other researchers have expanded the detection of affectwithin educational contexts to leverage physiological markers (Hussain, AlZoubi, Calvo, & D’Mello, 2011;Chang, Nelson, Pant & Mostow, 2013).Especially when dealing with web-based and tutoring activities, identifying the intensity and the time-occurrence of the emotional state is an important clue to distinguish an affective learning process froma pleasant, but not learning-effective, computer-based activity. Seeking clarity on this distinction,Muldner, Burleson, and VanLehn (2010), used physiological (skin conductance sensor and pupildilatation), behavioural (speaking aloud protocol, posture in the chair, and mouse clicks) and task-related data to predicted moments of excitement associated to learning, referred to as a “yes!”moment. They found that the “yes!” moment was associated with more reasoning, effort, andinvestment in solving the task, suggesting that the intensity of this positive emotion after theachievement of a goal may be a predictor of increased learning.This same physiological approach is also useful to identify negative feelings and reactions, which in turnis associated with lower performance in cognitive tasks. An increase in physiological reactivity wasobserved by Lunn and Harper (2010) to be associated with a frustrating web-based activity. Moreover,Choi et al. (2010) demonstrated that tense emotions induced by an external stimulus have a negativeeffect on performance in a subsequent cognitive task.The various studies of student affect emphasize the potential for empowering educators throughstudent sentiment awareness. Using one, or more, of the modalities of speech, psychophysiologicalmarkers, and computer vision, researchers are able to better understand the relationship betweenaffect and learning, and at a much more detailed level.2.7 Neurophysiological MarkersThough briefly mentioned in the previous section, there is a growing cadre of researchers doing work onpsychophysiology, and its relationship to cognition and learning. Burt and Obradović (2013) provide an(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 230overview of this domain, while also pinpointing key areas for researchers to pay attention to when doingthis work. Other researchers, such as Stevens, Galloway, and Berka (2007), describe the IMMEX systemused to study the electroencephalograms (EEGs) of students as they participate in a computer-basedenvironment. In their work, they also present preliminary findings on the relationship between EEG andcognitive load, distraction, and engagement. One unexpected finding of the research was that even as astudent’s skill level increased, the workload remained the same. This unexpected result highlights one ofthe key affordances of these new multimodal modes of analysis: they may challenge researchers toquestion previously held assumptions or intuitions about student learning. The study of Stevens et al.(2007) is only one among a host of cutting-edge publications that examine cardiovascular physiology(Cowley, Ravaja, & Heikura, 2013), mid-frontal brain activity (Luft, Nolte, & Bhattacharya, 2013), andother connections between cognition and physiology (Burt & Obradović, 2013).Moreover, studies vary in the number of sensors used as well as in the types of analyses. Using a singlechannel portable EEG device, Chang, Nelson, Pant & Mostow (2013) were able to distinguish easy anddifficult sentences read by children and adults. In a more complex task, nine EEG channels were used toidentify differences from solutions created by students when solving a maze problem that requiredphysics concepts. Students with better solutions (reduced number of leans used) had higher thetapower in the frontal areas of the brain, which is related to mental effort, concentration, and attention(She et al., 2012). Neuroimaging techniques increased the comprehension about brain mechanismsinvolved in learning as well in learning disabilities. Understanding brain mechanisms required forcognitive processing and learning is important to either adapt learning methodologies to specific topicsor create interventions for students with specific needs.2.8 Eye Gaze AnalysisAnother area applicable to educational research is eye tracking and gaze analysis. While this technologyhas long been used within the field of research on consumer electronics and software usage, recentwork in a variety of learning environments has shown eye tracking can be useful for understandingstudent learning. One of the constructs more related to eye gaze is attention. For example, Gomes,Yassine, Worsley, and Blikstein (2013) captured eye-tracking data from high school students as theycompleted a collection of engineering design games. By using machine learning to cluster the studentsbased on their gaze patterns, the team identified that the highest performing students used very similarpatterns in where they looked, how longed they looked, and their level of systematicity.Data from eye tracking also helps to understand what kind of approaches are useful in helping studentsto enhance learning. Mason, Pluchino, Tornatora, and Ariasi (2013) demonstrated that using pictures ina scientific text is better than using only text. However, based in the number of fixations in the final partof the text, the authors conclude that using an abstract picture that represents the topic studied (physicsphenomena) appears to be more efficient, i.e., same performance but less cognitive load than using aconcrete illustration about the same topic,(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 231However, de Koning, Tabbers, Rikers, and Paas (2010) argue that looking at specific stimulus canrepresent the student’s shifting of attention to possible areas of interest, but does not always mean thatthey are learning. In their study, students looked longer and more often at an instructional animationwith cues compared to the same animation without cues, but the authors could not confirm that givingcues would reduce the student’s cognitive load or even increase conceptual understanding.Notwithstanding, the most promising use of eye-tracking technology in education has been to studysmall collaborative learning groups. The overall framework for this type of work is to synchronize twoeye-trackers and compute the number of times a particular group achieves joint visual attention (JVA).JVA has been studied extensively in a variety of disciplines (developmental psychology, communication,learning sciences) and is known as a strong predictor of a group’s quality of collaboration. Richardsonand Dale (2005), for instance, found that the degree of gaze recurrence between individual speaker–listener dyads (i.e., the proportion of alignment of their gazes) was correlated with the listeners’accuracy on comprehension questions. In a remote collaboration, Jermann, Mullins, Nüssli, andDillenbourg (2011) describe how “good” programmers tend to have a higher recurrence of joint visualattention when having productive interactions, compared to less proficient programmers. Additionally,recent work by Schneider and Pea (2013) suggests that JVA is not just a proxy for predictingcollaboration, but can also be influenced to improve communication between students. They designedan intervention in which students worked in pairs (in different rooms). In one condition, the twoparticipants could see each other’s gaze; in the other condition, no such augmentation was provided.Their task was to study a set of diagrams to learn about the human visual system. Those who could seethe gaze of their partner in real time on the screen achieved significantly higher learning gains and had ahigher quality of collaboration. Those findings highlight the potential of using gaze-awareness tools foraugmenting student interactions in various learning environments and settings. It should be noted thatthose examples are limited to remote collaborations. Schneider et al., (2015) extends this line of work toco-located settings. Using mobile eye-trackers and computer vision algorithms, they were able toreplicate the findings above: in a side-by-side collaboration, JVA was found to be a significant predictorof student learning gains and performance on a problem-solving task.Finally, Schneider and Pea (2014) are expanding what can be predicted when combining JVA, networkanalysis and machine learning. In this work, they describe networks where nodes represent visualfixations and edges represent saccades. Their findings suggest that when those networks characterize adyad (i.e., the size of a node represents the amount of joint visual attention on one particular area of thescreen), different properties of the network are associated with different facets of a good collaboration.For instance, the extent to which students reach consensus during a problem-solving task is associatedwith the average size of the strongly connected components of the graphs. They found that otherdimensions of a productive collaboration (sustaining mutual understanding, dialogue management,information pooling, reaching consensus, task division, task management, technical coordination,(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 232reciprocal interaction, individual task orientation) could similarly be predicted by applying machine-learning algorithms on the features of those graphs.These studies suggest interesting opportunities to understand and enhance collaborative learning usingeye-tracking data. More specifically, they provide new ways to study small-group visual coordinationand its relationship to productive learning strategies. Recent work is generalizing this line of inquiryacross various settings, which opens promising new doors for predicting and influencing collaborationamong students.2.9 Multimodal Integration and Multimodal InterfacesHaving considered several example modalities currently being used by researchers to study studentlearning individually, we now turn to a final example that entails analysis using multiple modalities. Aspreviously noted, Multimodal Learning Analytics also builds on the idea of multimodal integration andmultimodal interfaces. Multimodal integration is the synchronous alignment and combination of datafrom different modalities (or contexts) in order to get a clearer understanding of the learning cues thatstudents are producing. Worsley (2014) and Worsley and Blikstein (2014) discuss and employ variousmultimodal learning analytics techniques. Worsley (2014) considers the impact of using differentmultimodal data fusion approaches. Specifically, the paper highlights naïve fusion, low-level (or data-level) fusion and high-level (or quasi feature-level) fusion as having differing levels of utility, and as beingassociated with different underlying research questions. Naïve fusion was the label given to multimodalanalyses that built machine-learning classifiers from the summary statistic generated from each of thedata streams or features. In many cases, these features are first subjected to feature selection in orderto reduce the feature space down to something reasonable. Low-level fusion (or feature fusion) involvedsynchronizing the data at each time step and conducting analyses on the features after they have beenfused together. Finally, high-level fusion is described as extracting one of more semantic level featuresfrom one or more data streams before fusing them with the other data streams. An example of thiswould be to do gesture recognition or speech recognition before aligning the hand/wrist movementand/or audio channels with the other data sources available for analysis.In Worsley and Blikstein (2014), the authors present a multimodal comparison from a two-conditionexperiment, in which students worked in pairs to complete an engineering design challenge. By usinghand/wrist movement, electro-dermal activation, and voice activity detection, the authors were able toidentify a set of representative multimodal states that students used, and subsequently used thosestates to model each student’s design approach. Interestingly, students in the two experimentalconditions used markedly different approaches. In this way, then, the analysis served to reveal some ofthe behavioural differences associated with the two different experimental conditions. The analysis alsorevealed that the multimodal behaviours observed had clear correlations with prior work onepistemological framing (Russ, Lee, & Sherin, 2012).(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 233The strategies used in Worsley (2014) and Worsley and Blikstein (2014) represent a small fraction of thework being done in the multimodal analysis community, which spans a variety of complex approachesfor doing multimodal fusion at different levels of analysis, as well as using a variety of algorithms, datarepresentations, and strategies for training and testing said algorithms (see Song, Morency, & Davis,2012; Scherer et al., 2012; and Ngiam et al., 2011 for more details.) A particular challenge, however, isreconciling the complexities of these computational approaches with actionable ideas and theories forlearning.Taken together, the prior research points to a wealth of technology and methodologies that can be usedfor doing multimodal analysis of student learning across a diversity of environments. By studyinglearning through these different lenses we can better identify how students are changing, and makemore sense of their changes. Furthermore, multimodal analysis enables researchers to get far morenuanced and complex understandings of student learning processes, something that we have onlybegun to study at scale.3 CONCLUSIONIn this article, we have presented a review of the literature on what we have termed “multimodallearning analytics” — a set of techniques employing multiple sources of data (video, logs, text, artifacts,audio, gestures, biosensors) to examine learning in realistic, ecologically valid, social, mixed-medialearning environments.The incorporation of multimodal techniques, which are extensively used in the multimodal interactioncommunity, should enable researchers to examine unscripted, complex tasks in more holistic ways. Inparticular, we have focused on describing a set of modalities that have been the topic of multimodalanalysis for decades, as well as modalities that have recently emerged as new data streams throughwhich researchers can study human interaction and behaviour.REFERENCESAlvarado, C., & Davis, R. (2006). Dynamically constructed Bayes nets for multi-domain sketchunderstanding. Proceedings of the ACM SIGGRAPH 2006 Courses (A. 33).http://dx.doi.org/10.1145/1281500.1281544Alvarado, C., Oltmans, M., & Davis, R. (2002). A framework for multi-domain sketch recognition.Proceedings of AAAI Spring Symposium on Sketch Understanding, Palo Alto, California (pp. 1–8).Retrieved from http://rationale.csail.mit.edu/publications/Alvarado2002Framework.pdfAnthony, L., Yang, J., Koedinger, K. (2007). Adapting handwriting recognition for applications in algebralearning. Proceedings of the ACM Workshop on Educational Multimedia and MultimediaEducation (EMME 2007), 47–56. http://dx.doi.org/10.1145/1290144.1290153(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 234Baker, R., D’Mello, S. K., Rodrigo, M. M. T., Graesser, A. C. (2010). Better to be frustrated than bored:The incidence, persistence, and impact of learners’ cognitive-affective states during interactionswith three different computer-based learning environments. International Journal of Human–Computer Studies, 68(4), 223–241. http://dx.doi.org/10.1016/j.ijhcs.2009.12.003Barron, B., & Darling-Hammond, L. (2010). Prospects and challenges for inquiry-based approaches tolearning. In H. Dumont, D. Istance, & F. Benavides (Eds.), The nature of learning: Using researchto inspire practice (pp. 199-225). Paris: OECD.Beck, J. E., & Sison, J. (2006). Using knowledge tracing in a noisy environment to measure studentreading proficiencies. International Journal of Artificial Intelligence in Education, 16(2), 129–143.Burt, K. B., & Obradović, J. (2013). The construct of psychophysiological reactivity: Statistical andpsychometric issues. Developmental Review, 33(1), 29–57.http://dx.doi.org/10.1016/j.dr.2012.10.002Chang, K. M., Nelson, J., Pant, U., & Mostow, J. (2013). Toward exploiting EEG input in a readingtutor. International Journal of Artificial Intelligence in Education, 22(1), 19–38.http://dx.doi.org/10.1007/978-3-642-21869-9_31Chang, M., & Forbus, K. (2012). Using quantitative information to improve analogical matching betweensketches. In Proceedings of the 24th Annual Conference on Innovative Applications of ArtificialIntelligence (AAAI-12), Toronto, Canada, 2269–2274.Choi, M. H. C., Su-Jeong Lee, S. J. L., Jae-Woong Yang, J. W. Y., Ji-Hye Kim, J. H. K., Jin-Seung Choi, J. S. C.,Jang-Yeon Park, J. Y. P., ... & Dae-Woon Lim, D. W. L. (2010). Changes in cognitive performancedue to three types of emotional tension. International Journal of Bio-Science and Bio-Technology, 2(4), 23–28. http://dx.doi.org/10.1007/978-3-642-17622-7_26Conati, C., & Maclaren, H. (2009). Empirically building and evaluating a probabilistic model of user affect.User Modeling and User-Adapted Interaction, 19(3), 267–303.http://dx.doi.org/10.1007/s11257-009-9062-8Cowley, B., Ravaja, N., & Heikura, T. (2013). Cardiovascular physiology predicts learning effects in aserious game activity. Computers & Education, 60(1), 299–309.http://dx.doi.org/10.1016/j.compedu.2012.07.014Craig, S. D., D’Mello, S., Witherspoon, A., & Graesser, A. (2008). Emote aloud during learning withAutoTutor: Applying the facial action coding system to cognitive-affective states during learning.Cognition & Emotion, 22(5), 777–788. http://dx.doi.org/10.1080/02699930701516759D’Mello, S., Craig, S., Witherspoon, A., McDaniel, B., & Graesser, A. (2008). Automatic detection oflearner’s affect from conversational cues. User Modeling and User-Adapted Interaction, 18(1),45–80.http://dx.doi.org/10.1007/s11257-007-9037-6de Koning, B. B., Tabbers, H. K., Rikers, R. M., & Paas, F. (2010). Attention guidance in learning from acomplex animation: Seeing is understanding? Learning and Instruction, 20(2), 111–122.http://dx.doi.org/10.1016/j.learninstruc.2009.02.010Dewey, J. (1902). The school and society. Chicago, Il: University of Chicago Press.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 235Dym, C. L. (1999). Learning engineering: Design, languages, and experiences. Journal of EngineeringEducation, 88(2), 145–148. http://dx.doi.org/10.1002/j.2168-9830.1999.tb00425.xForbes-Riley, K., Rotaru, M., & Litman, J. (2009). The relative impact of student affect on performancemodels in a spoken dialogue tutoring system. User Modeling and User-Adapted Interaction(Special Issue on Affective Modeling and Adaptation), 18(1–2), 11–43.http://dx.doi.org/10.1007/s11257-007-9038-5Forbus, K., Usher, J., Lovett, A., Lockwood, K., & Wetzel, J. (2011). CogSketch: Sketch understanding forcognitive science research and for education. Topics in Cognitive Science, 3(4), 648–666.http://dx.doi.org/10.1111/j.1756-8765.2011.01149.xFreire, P. (1970). Pedagogia do Oprimido. Rio de Janeiro: Paz e Terra.Gomes, J. S., Yassine, M., Worsley, M., & Blikstein, P. (2013). Analysing engineering expertise of highschool students using eye tracking and multimodal learning analytics. In S. K. DʼMello, R. A.Calvo, & A. Olney (Eds.), Proceedings of the 6th International Conference on Educational DataMining (EDM 2013), (pp. 375–377). Retrieved fromhttp://www.educationaldatamining.org/EDM2013/papers/rn_paper_88.pdfHowison, M., Trninic, D., Reinholz, D., & Abrahamson, D. (2011). The Mathematical Imagery Trainer:from embodied interaction to conceptual learning. In G. Fitzpatrick, C. Gutwin, B. Begole, W. A.Kellogg & D. Tan (Eds.), Proceedings of the annual meeting of CHI: ACM Conference on HumanFactors in Computing Systems (CHI 2011), (Vol. \"Full Papers\", pp. 1989-1998).http://dx.doi.org/10.1145/1978942.1979230Hussain, M. S., AlZoubi, O., Calvo, R. A., & D’Mello, S. K. (2011). Affect detection from multichannelphysiology during learning sessions with AutoTutor. In G. Biswas, S. Bull, J. Kay, A. Mitrovic(Eds.), Proceedings of the 15th International Conference on Artificial Intelligence in Education(AIED), (pp. 131–138). http://dx.doi.org/10.1007/978-3-642-21869-9_19Jee, B., Gentner, D., Forbus, K., Sageman, B., & Uttal, D. (2009). Drawing on experience: Use of sketchingto evaluate knowledge of spatial scientific concepts. In N. Taatgen & H. van Rijn (Eds.),Proceedings of the 31st Annual Conference of the Cognitive Science Society (CogSci 2009), (pp.2499–2504). Retrieved from http://blog.silccenter.org/publications_pdfs/jee etal_CogSci2009_Final.pdfJermann, P., Mullins, D., Nüssli, M.-A., & Dillenbourg, P. (2011). Collaborative gaze footprints: Correlatesof interaction quality. In H. Spada, G. Stahl, N. Miyake, N. Law (Eds.), Proceedings of the 11thInternational Conference on Computer-Supported Collaborative Learning (CSCL 2011), (Vol.1, pp.184–191). Hong Kong: International Society of the Learning Sciences.Kirschner, P. A., Sweller, J., & Clark, R. E. (2006). Why minimal guidance during instruction does notwork: An analysis of the failure of constructivist, discovery, problem-based, experiential, andinquiry-based teaching. Educational Psychologist, 41(2), 75–86.http://dx.doi.org/10.1207/s15326985ep4102_1Klahr, D., & Nigam, M. (2004). The equivalence of learning paths in early science instruction.Psychological Science, 15(10), 661. http://dx.doi.org/10.1111/j.0956-7976.2004.00737.x(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 236Levy, F., & Murnane, R. J. (2004). The new division of labor: How computers are creating the next jobmarket. Princeton, NJ: Princeton University Press.Litman, D., Moore, J., Dzikovska, M., & Farrow, E. (2009). Using natural language processing to analyzetutorial dialogue corpora across domains and modalities. Proceedings of the 2009 conference onArtificial Intelligence in Education: Building Learning Systems that Care: From KnowledgeRepresentation to Affective Modelling (AIED ʼ09), (pp. 149–156). The Netherlands: IOS Press.Retrieved from http://people.cs.pitt.edu/~litman/paper_125.pdfLuft, C. D., Nolte, G., & Bhattacharya, J. (2013). High-learners present larger mid-frontal theta power andconnectivity in response to incorrect performance feedback. The Journal of Neuroscience, 33(5),2029–2038. http://dx.doi.org/10.1523/JNEUROSCI.2565-12.2013Lunn, D., & Harper, S. (2010). Using galvanic skin response measures to identify areas of frustration forolder web 2.0 users. Proceedings of the 2010 International Cross Disciplinary Conference on WebAccessibility (W4A ’10), (p. 34). http://dx.doi.org/10.1145/1805986.1806032Mason, L., Pluchino, P., Tornatora, M. C., & Ariasi, N. (2013). An eye-tracking study of learning fromscience text with concrete and abstract illustrations. The Journal of Experimental Education,81(3), 356–384.http://dx.doi.org/10.1080/00220973.2012.727885Montessori, M. (1965). Spontaneous activity in education. New York: Schocken Books.Muldner, K., Burleson, W., & VanLehn, K. (2010). “Yes!” Using tutor and sensor data to predict momentsof delight during instructional activities. In P. de Bra, A. Kobsa, D. Chin (Eds.), Proceedings of18th International Conference, UMAP 2010: User modeling, adaptation, and personalization(Lecture Notes in Computer Science Series), (Vol. 6075, pp. 159–170).http://dx.doi.org/10.1007/978-3-642-13470-8_16Munteanu, C., Penn, G., & Zhu, X. (2009). Improving automatic speech recognition for lectures throughtransformation-based rules learned from minimal data. Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP (Vol. 2, Vol. 2), (pp.764–772). Stroudsburg, PA: ACM.Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., & Ng, A. Y. (2011). Multimodal deep learning. Proceedingsof the 28th International Conference on Machine Learning (ICML-11), (pp. 689–696). Retrieved from http://machinelearning.wustl.edu/mlpapers/papers/ICML2011Ngiam_399Ocumpaugh, J., Baker, R. S. J. d., Rodrigo, M. M. T. (2012). Baker-Rodrigo Observation Method Protocol(BROMP) 1.0: Training Manual. Version 1.0. [Technical Report]. New York: EdLab. Manila,Philippines: Ateneo Laboratory for the Learning Sciences.Papert, S. (1980). Mindstorms: Children, computers, and powerful ideas. New York: Basic Books.Pardos, Z. A., Baker, .R. S. J. d., San Pedro, M. O. C. Z., Gowda, S. M., Gowda, S. M. (2013). Affectivestates and state tests: Investigating how affect throughout the school year predicts end of yearlearning outcomes. Proceedings of the 3rd International Conference on Learning Analytics andKnowledge (LAK ’13), 117–124. http://dx.doi.org/10.1145/2460296.2460320(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 237Raca, M., Tormey, R., & Dillenbourg, P. (2014). Sleepers’ lag-study on motion and attention. Proceedingsof the 4th International Conference on Learning Analytics and Knowledge (LAK ʼ14), 36–43.http://dx.doi.org/10.1145/2567574.2567581Read, J. C. (2007). A study of the usability of handwriting recognition for text entry by children.Interacting with Computers, 19(1), 57–69. http://dx.doi.org/10.1007/1-84628-062-1_9Richardson, D. C., & Dale, R. (2005). Looking to understand: The coupling between speakers’ andlisteners’ eye movements and its relationship to discourse comprehension. Cognitive science,29(6), 1045–1060. http://dx.doi.org/10.1207/s15516709cog0000_29Russ, R. S., Lee, V. R., & Sherin, B. L. (2012). Framing in cognitive clinical interviews about intuitivescience knowledge: Dynamic student understandings of the discourse interaction. ScienceEducation, 96(4), 573–599. http://dx.doi.org/10.1002/sce.21014Scherer, S., Glodek, M., Layher, G., Schels, M., Schmidt, M., Brosch, T., ... & Palm, G. (2012). A genericframework for the inference of user states in human computer interaction. Journal onMultimodal User Interfaces, 6(3–4), 117–141.Schick, A. Morlock, D., Amma, C., Schultz, T., & Stiefelhagen, R. (2012). Vision-based handwritingrecognition for unrestricted text input in mid-air. Proceedings of the 14th ACM InternationalConference on Multimodal Interaction (ICMI ’12), 217–220. New York: ACM.http://dx.doi.org/10.1145/2388676.2388719Schlömer, T., Poppinga, B., Henze, N., & Boll, S. (2008). Gesture recognition with a Wii controller.Proceedings of the 2nd International Conference on Tangible and Embedded Interaction (TEI ’08),11–14. http://dx.doi.org/10.1145/1347390.1347395Schneider, B., & Blikstein, P. (2014). Unraveling Students’ Interaction Around a Tangible Interface usingGesture Recognition. In J. Stamper, Z. Pardos, M. Mavrikis, B. Mclauren (Eds.), Proceedings ofthe 7th International Conference on Educational Data Mining (EDM’14), (pp.320-323). Retrievedfrom http://educationaldatamining.org/EDM2014/proceedings/EDM2014Proceedings.pdfSchneider, B., & Pea, R. (2013). Real-time mutual gaze perception enhances collaborative learning andcollaboration quality. International Journal of Computer-Supported Collaborative Learning, 8(4),375–397. http://dx.doi.org/10.1007/s11412-013-9181-4Schneider, B., & Pea, R. (2014). Toward Collaboration Sensing. International Journal of Computer-Supported Collaborative learning, 9(4), 371-395. http://dx.doi.org/10.1007/s11412-014-9202-ySchneider, B., Sharma, K., Cuendet, S., Zufferey, G., Dillenbourg, P., & Pea, A. D. (2015). 3D tangiblesfacilitate joint visual attention in dyads. In O. Lindwall, P. Hakkinen, T. Koschmann, P.Tschounikine, S. Ludvigsen (Eds.), In Proceedings of the International Conference on ComputerSupported Collaborative Learning 2015: Exploring the Material Conditions of Learning (CSCL’15),(Vol.1, pp. 158–165). Gothenburg, Sweden: The International Society of the Learning Sciences.She, H. C., Jung, T. P., Chou, W. C., Huang, L. Y., Wang, C. Y., & Lin, G. Y. (2012). EEG dynamics reflect thedistinct cognitive process of optic problem solving. PLOS ONE, 7(7), e40731.http://dx.doi.org/10.1371/journal.pone.0040731(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 238Sherin, B. (2013). A computational study of commonsense science: An exploration in the automatedanalysis of clinical interview data. Journal of the Learning Sciences, 22(4), 600–638.http://dx.doi.org/10.1080/10508406.2013.836654Song, Y., Morency, L. P., & Davis, R. (2012). Multimodal human behavior analysis: Learning correlationand interaction across modalities. Proceedings of the 14th ACM International Conference onMultimodal Interaction (ICMI ’12), 27–30. http://dx.doi.org/10.1145/2388676.2388684Stevens, R. H., Galloway, T., & Berka, C. (2007). EEG-related changes in cognitive workload, engagementand distraction as students acquire problem solving skills. In C. Conati, K. McCoy, G. Paliouras(Eds.), Proceedings of the 11th International Conference on User Modeling, (pp. 187–196). BerlinHeidelberg: Springer Link. http://dx.doi.org/10.1007/978-3-540-73078-1_22Weinland, D., Ronfard, R., & Boyer, E. (2006). Free viewpoint action recognition using motion historyvolumes. Computer Vision and Image Understanding, 104(2), 249–257.http://dx.doi.org/10.1016/j.cviu.2006.07.013Worsley, M. (2014). Multimodal learning analytics as a tool for bridging learning theory and complexlearning behaviors. Proceedings of the 2014 ACM workshop on Multimodal Learning AnalyticsWorkshop and Grand Challenge (MLA ’14), 1–4. http://dx.doi.org/10.1145/2666633.2666634Worsley, M., & Blikstein, P. (2011). What’s an expert? Using learning analytics to identify emergentmarkers of expertise through automated speech, sentiment and sketch analysis. In M.Pechenizkiy, T. Calders, C. Conati, S. Ventura, C. Romero, & J. Stamper (Eds.), Proceedings of the4th Annual Conference on Educational Data Mining (EDM 2011), (pp. 235–240).Worsley, M., & Blikstein, P. (2013). Toward the development of multimodal action based assessment.Proceedings of the 3rd International Conference on Learning Analytics and Knowledge (LAK ’13),94–101. http://dx.doi.org/10.1145/2460296.2460315Worsley, M., & Blikstein, P. (2014). Deciphering the practices and affordances of different reasoningstrategies through multimodal learning analytics. Proceedings of the 2014 ACM workshop onMultimodal Learning Analytics Workshop and Grand Challenge (MLA ’14), 21–27.http://dx.doi.org/10.1145/2666633.2666637Worsley, M., & Blikstein, P. (2015). Using learning analytics to study cognitive disequilibrium in acomplex learning environment. Proceedings of the 5th International Conference on LearningAnalytics and Knowledge (LAK ʼ15), 426–427. http://dx.doi.org/10.1145/2723576.2723659Yilmaz, A., & Shah, M. (2005). Actions sketch: A novel action representation. In C. Schmid, S. Soatto, C.Tomasi (Eds.), Proceedings of the 2005 IEEE Computer Society Conference on Computer Visionand Pattern Recognition (CVPR 2005), (Vol. 1, pp. 984–989). Los Alamitos, CA: IEEE ComputerSociety. http://dx.doi.org/10.1109/CVPR.2005.58Zaïane, O. R. (2001). Web usage mining for a better web-based learning environment. In Proceedings ofthe Conference on Computers and Advanced Technology in Education (CATE 2001), (pp. 60–64).https://webdocs.cs.ualberta.ca/~zaiane/postscript/CATE2001.pdf"

  // recordDocument(
  //   'testing_document',
  //   "(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 220Multimodal Learning Analytics and Education Data Mining: UsingComputational Technologies to Measure Complex Learning TasksPaulo BliksteinGraduate School of Education and (by courtesy) Computer Science DepartmentStanford University, USApaulob@stanford.eduMarcelo WorsleyLearning Sciences & Computer ScienceNorthwestern University, USAmarcelo.worsley@northwestern.eduABSTRACT: New high-frequency multimodal data collection technologies and machine learninganalysis techniques could offer new insights into learning, especially when students have theopportunity to generate unique, personalized artifacts, such as computer programs, robots, andsolutions engineering challenges. To date most of the work on learning analytics and educationaldata mining has been focused on online courses and cognitive tutors, both of which provide ahigh degree of structure to the tasks, and are restricted to interactions that occur in front of acomputer screen. In this paper, we argue that multimodal learning analytics can offer newinsights into student learning trajectories in more complex and open-ended learningenvironments. We present several examples of this work and its educational applications.Keywords: Learning analytics, multimodal interaction, constructivism, constructionism,assessment1 INTRODUCTIONThe same battle is fought in every field of educational research and practice: the champions of the directinstruction of well-defined content pitted against those who encourage student-centred exploration ofill-defined domains. These wars have taken place repeatedly over past decades, and partisans on eachside have been reborn in multiple incarnations. The first tradition tends to be aligned with behaviouristor neo-behaviourist approaches, while the second favours constructivist-inspired pedagogies. Inlanguage arts, the battle has been between phonics and the whole word approach. In math, war iswagged between teaching algorithms versus instruction in how to think mathematically. In history, theyclash over the relative merits of critical interpretations and the memorization of historical facts. Inscience, they clash about inquiry-based approaches versus direct instruction of formulas and principles.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 221The educational research community has always maintained that the debate would end when researchresults inevitably demonstrated the superiority of one of the sides. Yet this conclusion has eludedscholarship for decades. One of the reasons for this interminable contest is that the underlying rationalefor the differences concerns individual values and societal beliefs and will not be resolved by a purelyscientific approach. In fact, the whole debate may serve the educational research community in quite adifferent capacity. More specifically the debates may reveal the underlying visions for what educationshould be about, for different groups, and we might more profitably re-examine the nature and purposeof our schools. More fundamentally, is education a tool for filtering, ranking, emancipation, socialequalization, economic progress, meritocracy, or for the promotion of social Darwinism?Educational scholars would greatly differ in their answers — rendering the question of “which approachis better,” and “what evidence suffices,” pointless. As with the debates on public healthcare and fiscalpolicy, despite our best efforts to generate reliable research, the “best” way to conduct education willalways be controversial and dependent on larger societal and political winds. But the fundamentalproblem, and the motivation for this article, is that the prevailing issue is not who “wins” the debate, butrather the existence of a healthy debate. Fostering a healthy debate requires some level of symmetry.However, as it stands, the playing field is not symmetrical. The “direct instruction” approach isinherently easier to test and quantify using currently available tools that include mass-production ofcontent and decades of research concerning psychometrics and standardized testing strategies.Meanwhile, the constructivist side counts on laborious interventions, and complex mixed-moderesearch methods. The result of this asymmetry is that public systems, more dependent on high-profileresearch results, are left, by inertia, to the designs of the proponents of traditional approaches, whileonly affluent schools, private or public, who can experiment more, can afford to implement modern,constructivist approaches to learning.Learning analytics could deepen this asymmetry, or help eliminate it. The elimination of the asymmetrycould re-establish a healthy public debate around education, where both sides would have comparableand credible results to show, and policy makers would be able to make choices based on their valuesand visions for education. However, the deepening of this asymmetry could be a significant impedimentto progressive education and the vision of creating alternative learning environments that can reach amore diverse population of learners. Should public education succumb to the temptation of the fiscalbenefits supposedly offered by total automatization and its much lower baseline for cost and quality, allother options would be driven into the ground as economically unfeasible: who could compete withvirtually free computerized tutors and videos? How many years would the debate take, while childrencaught in the “experimental” years are being victimized?Consequently, we propose that an important goal of learning analytics is to equalize the playing field bydeveloping methods that examine and quantify non-standardized forms of learning. We suggest thatthis need for a level playing field is more necessary than ever, given the increasing demand for scalableproject-based, interest-driven learning and student-centred pedagogies (e.g., Papert, 1980). Within our(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 222increasingly interconnected societal and economic environment — which has become pervaded bytechnology and threatened by challenging global problems such as climate change — both K–12 anduniversity-level engineering education (Dym, 1999) demand higher-level, complex problem-solving asopposed to performance in routine cognitive tasks (Levy & Murnane, 2004). Approaches that placepremiums on student-centred, constructivist, self-motivated, self-directed learning have beenadvocated for decades (e.g., Dewey, 1902; Freire, 1970; Montessori, 1965; Barron & Darling-Hammond,2010) but have failed to become scalable and prevalent, and have come under attack during the lastdecade (e.g., Kirschner, Sweller, & Clark, 2006; Klahr & Nigam, 2004).New high-frequency data collection technologies and machine learning could offer new insights intolearning in tasks in which students are allowed to generate unique, personalized artifacts, such ascomputer programs, robots, movies, animations, and solutions to engineering challenges. To date mostof the work on learning analytics and educational data mining has focused on tasks that are computer-mediated and are more structured and scripted. In this work, we argue that multimodal data collectionand analysis techniques (“multimodal learning analytics” or MMLA) could yield novel methods thatgenerate distinctive insights into what happens when students create unique solution paths toproblems, interact with peers, and act in both the physical and digital worlds.Assessment and feedback is particularly difficult within these open-ended environments, and theselimitations have hampered many attempts to make such approaches more prevalent. Automated, fine-grained data collection and analysis could help resolve this tension in two ways. First, such capacitieswould give researchers tools to examine student-centred learning in unprecedented scale and detail.Second, these techniques could improve the scalability of these pedagogies since they make feasibleboth assessment and formative feedback, which are typically very complex and laborious in suchenvironments. They might not only reveal students’ trajectories throughout specific learning activities,but they could also help researchers design better supports, pedagogical approaches, and learningmaterials.At the same time, in the well-established field of multimodal interaction, new data collection andsensing technologies are making it possible to capture massive amounts of data in all fields of humanactivity. These technologies include the logging of computer activities, wearable cameras, wearablesensors, biosensors (e.g., that permit measurements of skin conductivity, heartbeat, andelectroencephalography), gesture sensing, infrared imaging, and eye tracking. Such techniques enableresearchers to have unprecedented insight into the minute-by-minute development of a number ofactivities, especially those involving multiple dimensions of activity and social interaction. However, thetechnologies just mentioned have not yet become popular in the field of learning analytics. We proposethat multimodal learning analytics could bring together these multiple techniques in morecomprehensive evaluations of complex cognitive abilities, especially in environments where theprocesses or outcomes are unscripted. Thus, the goal of this paper is to demonstrate the feasibility andpower of novel assessment techniques in several modalities and learning contexts.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 2232 STATE OF THE FIELDIn considering the current sensing and assessment modalities possible using MMLA, we see three non-mutually exclusive areas: assessing student knowledge, assessing student affect and physiology, andassessing student intentions or beliefs. At the crux of all these forms of student characterization is theunderlying invocation of data analysis to generate useful models from large sets of quantitative data.Hence, what varies in the different forms of student assessment is the source of the raw data and howthat data is translated into computable data. Once the translation has been completed, the data isprocessed using a collection of machine learning algorithms. In what follows, we present severalmethods being used to capture and process student data. There are several techniques — web datamining, user data mining, simple web-based surveys, etc. — but the following technologies have beenselected for inclusion because they live on the cutting-edge of technology and help promote the notionof “natural” assessment (Zaïane, 2001). Furthermore, while each of these technologies represents aresearch contribution in and of itself, our interest in including them is to bring to the forefront a widervariety of non-traditional approaches that education researchers and educational data scientists canbegin to combine in their learning analytics research. For the first three techniques we mention — textanalysis, speech analysis, and handwriting analysis — our discussion will be very cursory, as theserepresent areas of research that have received considerable attention with the computer sciencecommunity, and have started to get traction within the learning analytics community. Nonetheless, wewant to make the reader aware of some of the current capabilities and research in these areas. For thelatter analyses that we discuss, we will engage in a more detailed and descriptive explanation of each, asthese domains remain relatively new, even among the computer science community.2.1 Text AnalysisWhile text analysis, or natural language processing, has been around for decades it is only in recenthistory that education has begun to benefit from this technology, and researchers have targetedlearners’ text explicitly. Despite the fact that text itself is not multimodal, text analysis allows for theinterpretation of open-ended writing tasks, differently from multiple-choice tests. Given that collectingtext from students is unproblematic both technically and logistically, it constitutes one of the mostpromising modalities for MMLA: text can be easily gathered from face-to-face and online activities, fromtests and exams, and from expert-generated prose from textbooks and online sources (often used asbaseline). For example, Sherin (2013) has been doing pioneering work in the analysis of text in thelearning sciences community. He uses techniques from topic modelling and clustering to study theprogression of students’ ideas and intuitions as they describe the explanation for the existence of thefour seasons (Sherin, 2013). More specifically, he shows that, as students explain the seasons, invokingdifferent types of scientific explanations, it is possible to identify which type of explanation each studentis referring to at different points in time. He also goes beyond this to show how students can beaccurately clustered without using a pre-defined set of exemplar responses, but instead by usingautomatically derived topics models from the corpus itself. This approach of clustering segments of(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 224students’ text based on the descriptions of their peers is a powerful tool that can allow researchers andpractitioners to draw meaningful commonalities and differences among large populations of students,without having to explicitly read and compare the entirety of each transcript. Given the prevalence oftext-based assessment and the intensive use of text in face-to-face and online learning, this promisingmethod will likely accelerate discourse-based research, and open new possibilities for large-scaleanalysis of open-ended text corpora.2.2 Speech AnalysisSpeech analysis shares many of same goals and tools as text analysis. Speech analysis, however, furtherremoves the student from the traditional assessment setting by allowing them to demonstrate fluencyin a more natural setting. For example, Worsley & Blikstein (2011) studied how elements of studentspeech, as inferred by linguistic, textual, and prosodic features, can be predictive for identifyingstudents’ level of expertise on open-ended engineering design tasks. In addition to traditional linguisticand prosodic features, speech signals can be analyzed for a wealth of other characteristics. Variousresearch tools have been developed to help researchers in the process of extracting these features,however, several challenges remain in knowing how to analyze student learning appropriately using saidfeatures.Other researchers have moved away from raw analysis of the speech signal to leverage speechrecognition capabilities. In particular, Beck and Sison (2006) demonstrated a method for using speechrecognition to assess reading proficiency. As an extension of Project LISTEN — an intelligent tutor thathelps elementary school students improve their reading skills — researchers completed a study thatcombines speech recognition with knowledge tracing, a form of probabilistic monitoring. By having alanguage model that was largely restricted to the content of each book being learned, the work requiredfor doing automatic speech recognition, and subsequent accuracy classification, was greatly simplified.Outside of the education domain there have been decades of work in developing speech recognizersand dialogue managers. However, to date, such technologies are still not widely used in educationbecause of the challenges associated with building a satisfactory language model that can reliablyrecognize speech. Munteanu, Peng, and Zhu (2009) have made some progress in this area by showinghow to improve speech recognition of lectures in college-level STEM class. A primary consideration inthe area of speech recognition, therefore, will be to identify the most effective ways to use thistechnology in real-world educational settings. Although using it to transcribe lectures might be feasible,the challenge of collecting and interpreting student data seems extremely difficult. Differently fromother applications of speech recognition (smartphones, personal assistants, dictation), educationalapplications need to address simultaneously classroom noise, multiple overlapping speakers, andlogistical difficulties in voice training — very ambitious challenges that have not been solved yet.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 2252.3 Handwriting AnalysisA different form of text analysis is handwriting analysis, which is important in educational settingsbecause a considerable part of the work done by students is still handwritten. Anthony, Yang, andKoedinger (2007) highlight the affordances of combining handwriting recognition with intelligent tutorsfor algebra. Based on their study of high school and middle school students, introducing handwritingrecognition halved the time students needed to complete tutoring activities because students no longerhad to deal with cumbersome keyboard and mouse-based entry. This is significant because it enablesstudents to focus on understanding the material using familiar forms of interaction as opposed tostruggling to learn a new interface. Accordingly, handwriting recognition can facilitate more effectivelearning by eliminating the barriers to using certain computer-based interfaces. It also permits thestudent to learn in such a way that more closely parallels the usual mathematics environment (i.e.,utilizing a writing tool as opposed to a keyboard), which may increase transfer.Researchers also studied the use of handwriting recognition technology among school-aged children(Read, 2007), examining the length and quality of stories produced by students using different inputmethods. A primary finding of this work was that students were more willing to engage in the writingprocess when using digital ink than when using traditional keyboard input. However, the team still foundthat handwriting recognition technology was not yet comparable to traditional paper and pencil.Similarly to Anthony et al. (2007), Read (2007) emphasizes the affordances of handwriting as a morenatural form of authorship that may help students better engage in learning.More recent work extends handwriting recognition to mid-air “writing” that achieves high levels ofaccuracy by utilizing a combination of computer vision, multiple cameras, and machine learning (Schick,Morlock, Amma, Schultz, & Stiefelhagen, 2012). This approach highlights some of the more recentopportunities in handwriting recognition in novel learning environments and contributes to thediscussion around the expansive possibilities available away from traditional keyboards and screens.2.4 Sketch AnalysisWhereas handwriting analysis is primarily concerned with looking for words, others researchers haveembarked on work that looks at both text-based and graphic-based representations. Fundamental workon object recognition and sketches is that of Alvarado, Oltmans, and Davis (2002) and Alvarado andDavis (2006). These researchers developed a framework for performing multi-domain recognition ofsketches using Bayesian networks and a predetermined set of shapes and patterns for each domain.With the predefined shapes and patterns, their algorithm is able to decipher messy sketches from thedomains of interest.Ken Forbus and colleagues also describe seminal work in the development of both systems andtechniques for analyzing and comparing sketches among learners. For example, Jee, Gentner, Forbus,(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 226Sageman, & Uttal (2009) explain the design and implementation of CogSketch, a tool used to study howstudents of different levels of experience describe common scientific concepts in geology throughsketches (Forbus, Usher, Lovett, Lockwood, & Wetzel, 2011). CogSketch pays particular attention to boththe content and the process of the sketches being developed. Chang and Forbus (2012) extend this workon qualitative sketching to include quantitative analysis of sketching, which allows them to garner amore accurate representation and understanding of the sketches.Sketching is particularly important given the current focus on conceptual learning in STEM. One of themost popular forms of eliciting student knowledge in science has been the creation of diagrams andconcept maps. From this prior work, it is apparent that a number of research groups have demonstratedthe ability to do meaningful analyses of sketches in order to study cognition and learning.2.5 Action and Gesture AnalysisAction recognition has recently received considerable attention within the computer vision community.For example, work by Weinland, Ronfard, & Boyer (2006) and Yilmaz and Shah (2005), among others,has demonstrated the ability to detect basic human actions related to movement. The work of Weinlandet al. (2006) involved developing a technique that could capture user actions independent of gender,body size, and viewpoint. The work of Yilmaz and Shah (2005) involved human action recognition usinguncalibrated moving cameras, which might prove useful for the dynamic nature of classrooms and/orlaboratories.This kind of work is currently being applied to classroom settings as well. Raca, Tormey, and Dillenbourg(2014), for instance, are pioneering ways of capturing student engagement and attention by conductingframe-by-frame analyses of videos taken from the teacher’s position. They show that students’ motionand level of attention can be estimated using computer vision, and that individuals with lower levels ofattention are slower to react than focused students. This line of work opens the door for new kinds offeedback loops for teachers, by providing not only real-time information about students but alsoaggregate measures of their levels of attention over time.Other work in the area of gesture recognition has leveraged infrared cameras and accelerometers thatare affixed to the research subject. Using infrared, one avoids some of the complications that may existwith camera geometry, lighting, and other forms of visual variance. Using this approach Schlömer,Poppinga, Henze, and Boll (2008) demonstrate the ability to construct a gesture recognition system bycapturing and processing accelerometer data from a Nintendo Wiimote. Their technique allows them toreliably capture gestures for squares, circles, rolling, the shape “Z,” etc.More recent work has taken advantages of the Microsoft Kinect sensor and simple infrared detectors aslow cost tools for capturing and studying human gestures. The Mathematical Imagery Trainer (Howison,Trninic, Reinholz, & Abrahamson, 2011) uses hand gestures captured by the Kinect sensor as a way forstudying student understanding of proportions. Students use their hands to indicate the relationship(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 227between two values, and benefit from visual feedback on the correctness of their hand placement. Thissystem also enables teachers to give students real-time, immediate feedback and change theirinstruction as they perceive students’ difficulties (and not only after the fact), and points to one of themain benefits of multimodal learning analytics. As an even more basic example, the Kinect sensor can beused to give teachers and students immediate feedback about the amount of gesticulation that they aredoing. Hence, without requiring a set of recommended actions, low-cost sensing of movement could beuseful in helping students and teachers be more aware of their own behaviours.Related to the measurement of student gesticulation, early work in this domain by Worsley and Blikstein(2013) involved a comparison of hand/wrist movement between experts and novices as they completedan engineering design task. In particular, the researchers used hand/wrist movement data from a Kinectsensor to examine the extent of two-handed action, and found that experts were much more likely toemploy two-handed actions than novices. These preliminary results aligned with theories associatedwith two-handed inter-hemispheric actions, and provided initial motivation for studying gestures incomplex learning environments.In a similar line of work, Schneider and Blikstein (2014) used a Kinect sensor to evaluate studentstrategies when interacting with a Tangible User Interface (TUI): their task was to learn about the humanhearing system by interacting with 3D-printed organs of the inner ear. Using clustering algorithms, theauthors found that students’ body postures fell into three prototypical positions (Figure 1): an active,semi-active, or passive state. The amount of time spent in the active state was significantly correlatedwith higher learning gains, and the time spent in the passive state was significantly correlated with lowerlearning gains. More interestingly, the number of transitions between those states was the strongestpredictor of learning. Those results suggest that successful students went through cycles of reflectionand action, which helped them gain a deeper understanding of the domain taught. This approach showsthe potential of using clustering methods on gestures data to find recurring behaviours associated withhigher learning gain.Figure 1: Using k-means on student body posture (Schneider & Blikstein, 2014). The first state (left)is active, with both hands on the table; the second (middle) is passive, with arms crossed; the third(right) is semi-active, with only one hand on the table.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 228As a whole, the advances in action and gesture recognition, and the introduction of low-cost, high-accuracy sensors is creating additional opportunities for action and gesture recognition to be included ineducation research.2.6 Affective State AnalysisStudying students’ affective states can often be challenging and hard to validate. However, severalstudies have demonstrated that identifying affect can be done consistently, and that affect is animportant marker in studying and understanding learning.2.6.1 Human Annotated Affective StatesBaker, D’Mello, Rodrigo, & Graesser (2010) and Pardos, Baker, San Pedro, Gowda, & Gowda (2013) areexamples of work using human annotated affective states. In Pardos et al. (2013), the researchers usedthe Baker-Rodrigo Observation Method Protocol (BROMP) (Ocumpaugh, Baker, & Rodrigo, 2012) tocorrelate student behaviour and affect while participating in cognitive tutoring activities withperformance on standardized tests. They found that the learning gains associated with certain affectivestates, namely boredom and confusion, are highly dependent on the level of scaffolding that the studentis receiving. This finding builds on prior work that studies affective state as students participate incognitive tutoring activities (e.g., Litman, Moore, Dzikovska, & Farrow, 2009; Forbes-Riley, Rotaru, &Litman, 2009).2.6.2 Automatically Annotated Affective StateOther work, using the Facial Action Coding System (FACS), has demonstrated that researchers canrecognize student affective state by simply observing their facial expressions. In the case of Craig,D’Mello, Witherspoon, and Graesser (2008), researchers were able to perceive boredom, stress, andconfusion by applying machine learning to video data of the student’s face throughout the tutoringexperience. Data was collected while students interacted with AutoTutor, an intelligent tutoring systemfor learning science. The technique that Craig et al. (2008) validated is a highly non-invasive mechanismfor realizing student sentiment, and can be coupled with computer vision technology to enablemachines to detect changes in emotional state or cognitive-affect automatically. Worsley and Blikstein(2015) utilize the Facial Action Coding System to compare two different experimental conditions. Morespecifically, the authors compared the frequency and rate of transitions among four automaticallyderived affective states that are conjectured to be important for learning. In particular, they were ableto show that the two experimental conditions expressed significantly different rates of confusion anddiffered in how frequently they transitioned from neutral to surprise, and from neutral to confusion.Being in, or transitioning to a confused expression was generally associated with good outcomes,whereas being more surprised, or transitioning to an expression of surprise was generally associatedwith less favourable outcomes.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 229Researchers have also used conversational cues to realize students’ emotional states. Similar to Craig etal. (2008), D’Mello, Craig, Witherspoon, McDaniel, & Graesser (2008) designed an application that coulduse spoken dialogue to recognize the states of boredom, frustration, flow, and confusion. Researcherswere able to resolve the validity of their findings through comparison to emote-aloud (a derivative oftalk-aloud where participants describe their emotions as they feel them) activities while studentsinteracted with AutoTutor.2.6.3 Physiological Markers of Affective StateMore recent work in this space was able to accurately predict the affective state, and the source of thechange in affective state for users as they interact with a computer-based tutoring system (Conati &MacLaren, 2009). In particular, the system was able to predict when students experienced joy, distress,and admiration effectively. In the past years, other researchers have expanded the detection of affectwithin educational contexts to leverage physiological markers (Hussain, AlZoubi, Calvo, & D’Mello, 2011;Chang, Nelson, Pant & Mostow, 2013).Especially when dealing with web-based and tutoring activities, identifying the intensity and the time-occurrence of the emotional state is an important clue to distinguish an affective learning process froma pleasant, but not learning-effective, computer-based activity. Seeking clarity on this distinction,Muldner, Burleson, and VanLehn (2010), used physiological (skin conductance sensor and pupildilatation), behavioural (speaking aloud protocol, posture in the chair, and mouse clicks) and task-related data to predicted moments of excitement associated to learning, referred to as a “yes!”moment. They found that the “yes!” moment was associated with more reasoning, effort, andinvestment in solving the task, suggesting that the intensity of this positive emotion after theachievement of a goal may be a predictor of increased learning.This same physiological approach is also useful to identify negative feelings and reactions, which in turnis associated with lower performance in cognitive tasks. An increase in physiological reactivity wasobserved by Lunn and Harper (2010) to be associated with a frustrating web-based activity. Moreover,Choi et al. (2010) demonstrated that tense emotions induced by an external stimulus have a negativeeffect on performance in a subsequent cognitive task.The various studies of student affect emphasize the potential for empowering educators throughstudent sentiment awareness. Using one, or more, of the modalities of speech, psychophysiologicalmarkers, and computer vision, researchers are able to better understand the relationship betweenaffect and learning, and at a much more detailed level.2.7 Neurophysiological MarkersThough briefly mentioned in the previous section, there is a growing cadre of researchers doing work onpsychophysiology, and its relationship to cognition and learning. Burt and Obradović (2013) provide an(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 230overview of this domain, while also pinpointing key areas for researchers to pay attention to when doingthis work. Other researchers, such as Stevens, Galloway, and Berka (2007), describe the IMMEX systemused to study the electroencephalograms (EEGs) of students as they participate in a computer-basedenvironment. In their work, they also present preliminary findings on the relationship between EEG andcognitive load, distraction, and engagement. One unexpected finding of the research was that even as astudent’s skill level increased, the workload remained the same. This unexpected result highlights one ofthe key affordances of these new multimodal modes of analysis: they may challenge researchers toquestion previously held assumptions or intuitions about student learning. The study of Stevens et al.(2007) is only one among a host of cutting-edge publications that examine cardiovascular physiology(Cowley, Ravaja, & Heikura, 2013), mid-frontal brain activity (Luft, Nolte, & Bhattacharya, 2013), andother connections between cognition and physiology (Burt & Obradović, 2013).Moreover, studies vary in the number of sensors used as well as in the types of analyses. Using a singlechannel portable EEG device, Chang, Nelson, Pant & Mostow (2013) were able to distinguish easy anddifficult sentences read by children and adults. In a more complex task, nine EEG channels were used toidentify differences from solutions created by students when solving a maze problem that requiredphysics concepts. Students with better solutions (reduced number of leans used) had higher thetapower in the frontal areas of the brain, which is related to mental effort, concentration, and attention(She et al., 2012). Neuroimaging techniques increased the comprehension about brain mechanismsinvolved in learning as well in learning disabilities. Understanding brain mechanisms required forcognitive processing and learning is important to either adapt learning methodologies to specific topicsor create interventions for students with specific needs.2.8 Eye Gaze AnalysisAnother area applicable to educational research is eye tracking and gaze analysis. While this technologyhas long been used within the field of research on consumer electronics and software usage, recentwork in a variety of learning environments has shown eye tracking can be useful for understandingstudent learning. One of the constructs more related to eye gaze is attention. For example, Gomes,Yassine, Worsley, and Blikstein (2013) captured eye-tracking data from high school students as theycompleted a collection of engineering design games. By using machine learning to cluster the studentsbased on their gaze patterns, the team identified that the highest performing students used very similarpatterns in where they looked, how longed they looked, and their level of systematicity.Data from eye tracking also helps to understand what kind of approaches are useful in helping studentsto enhance learning. Mason, Pluchino, Tornatora, and Ariasi (2013) demonstrated that using pictures ina scientific text is better than using only text. However, based in the number of fixations in the final partof the text, the authors conclude that using an abstract picture that represents the topic studied (physicsphenomena) appears to be more efficient, i.e., same performance but less cognitive load than using aconcrete illustration about the same topic,(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 231However, de Koning, Tabbers, Rikers, and Paas (2010) argue that looking at specific stimulus canrepresent the student’s shifting of attention to possible areas of interest, but does not always mean thatthey are learning. In their study, students looked longer and more often at an instructional animationwith cues compared to the same animation without cues, but the authors could not confirm that givingcues would reduce the student’s cognitive load or even increase conceptual understanding.Notwithstanding, the most promising use of eye-tracking technology in education has been to studysmall collaborative learning groups. The overall framework for this type of work is to synchronize twoeye-trackers and compute the number of times a particular group achieves joint visual attention (JVA).JVA has been studied extensively in a variety of disciplines (developmental psychology, communication,learning sciences) and is known as a strong predictor of a group’s quality of collaboration. Richardsonand Dale (2005), for instance, found that the degree of gaze recurrence between individual speaker–listener dyads (i.e., the proportion of alignment of their gazes) was correlated with the listeners’accuracy on comprehension questions. In a remote collaboration, Jermann, Mullins, Nüssli, andDillenbourg (2011) describe how “good” programmers tend to have a higher recurrence of joint visualattention when having productive interactions, compared to less proficient programmers. Additionally,recent work by Schneider and Pea (2013) suggests that JVA is not just a proxy for predictingcollaboration, but can also be influenced to improve communication between students. They designedan intervention in which students worked in pairs (in different rooms). In one condition, the twoparticipants could see each other’s gaze; in the other condition, no such augmentation was provided.Their task was to study a set of diagrams to learn about the human visual system. Those who could seethe gaze of their partner in real time on the screen achieved significantly higher learning gains and had ahigher quality of collaboration. Those findings highlight the potential of using gaze-awareness tools foraugmenting student interactions in various learning environments and settings. It should be noted thatthose examples are limited to remote collaborations. Schneider et al., (2015) extends this line of work toco-located settings. Using mobile eye-trackers and computer vision algorithms, they were able toreplicate the findings above: in a side-by-side collaboration, JVA was found to be a significant predictorof student learning gains and performance on a problem-solving task.Finally, Schneider and Pea (2014) are expanding what can be predicted when combining JVA, networkanalysis and machine learning. In this work, they describe networks where nodes represent visualfixations and edges represent saccades. Their findings suggest that when those networks characterize adyad (i.e., the size of a node represents the amount of joint visual attention on one particular area of thescreen), different properties of the network are associated with different facets of a good collaboration.For instance, the extent to which students reach consensus during a problem-solving task is associatedwith the average size of the strongly connected components of the graphs. They found that otherdimensions of a productive collaboration (sustaining mutual understanding, dialogue management,information pooling, reaching consensus, task division, task management, technical coordination,(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 232reciprocal interaction, individual task orientation) could similarly be predicted by applying machine-learning algorithms on the features of those graphs.These studies suggest interesting opportunities to understand and enhance collaborative learning usingeye-tracking data. More specifically, they provide new ways to study small-group visual coordinationand its relationship to productive learning strategies. Recent work is generalizing this line of inquiryacross various settings, which opens promising new doors for predicting and influencing collaborationamong students.2.9 Multimodal Integration and Multimodal InterfacesHaving considered several example modalities currently being used by researchers to study studentlearning individually, we now turn to a final example that entails analysis using multiple modalities. Aspreviously noted, Multimodal Learning Analytics also builds on the idea of multimodal integration andmultimodal interfaces. Multimodal integration is the synchronous alignment and combination of datafrom different modalities (or contexts) in order to get a clearer understanding of the learning cues thatstudents are producing. Worsley (2014) and Worsley and Blikstein (2014) discuss and employ variousmultimodal learning analytics techniques. Worsley (2014) considers the impact of using differentmultimodal data fusion approaches. Specifically, the paper highlights naïve fusion, low-level (or data-level) fusion and high-level (or quasi feature-level) fusion as having differing levels of utility, and as beingassociated with different underlying research questions. Naïve fusion was the label given to multimodalanalyses that built machine-learning classifiers from the summary statistic generated from each of thedata streams or features. In many cases, these features are first subjected to feature selection in orderto reduce the feature space down to something reasonable. Low-level fusion (or feature fusion) involvedsynchronizing the data at each time step and conducting analyses on the features after they have beenfused together. Finally, high-level fusion is described as extracting one of more semantic level featuresfrom one or more data streams before fusing them with the other data streams. An example of thiswould be to do gesture recognition or speech recognition before aligning the hand/wrist movementand/or audio channels with the other data sources available for analysis.In Worsley and Blikstein (2014), the authors present a multimodal comparison from a two-conditionexperiment, in which students worked in pairs to complete an engineering design challenge. By usinghand/wrist movement, electro-dermal activation, and voice activity detection, the authors were able toidentify a set of representative multimodal states that students used, and subsequently used thosestates to model each student’s design approach. Interestingly, students in the two experimentalconditions used markedly different approaches. In this way, then, the analysis served to reveal some ofthe behavioural differences associated with the two different experimental conditions. The analysis alsorevealed that the multimodal behaviours observed had clear correlations with prior work onepistemological framing (Russ, Lee, & Sherin, 2012).(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 233The strategies used in Worsley (2014) and Worsley and Blikstein (2014) represent a small fraction of thework being done in the multimodal analysis community, which spans a variety of complex approachesfor doing multimodal fusion at different levels of analysis, as well as using a variety of algorithms, datarepresentations, and strategies for training and testing said algorithms (see Song, Morency, & Davis,2012; Scherer et al., 2012; and Ngiam et al., 2011 for more details.) A particular challenge, however, isreconciling the complexities of these computational approaches with actionable ideas and theories forlearning.Taken together, the prior research points to a wealth of technology and methodologies that can be usedfor doing multimodal analysis of student learning across a diversity of environments. By studyinglearning through these different lenses we can better identify how students are changing, and makemore sense of their changes. Furthermore, multimodal analysis enables researchers to get far morenuanced and complex understandings of student learning processes, something that we have onlybegun to study at scale.3 CONCLUSIONIn this article, we have presented a review of the literature on what we have termed “multimodallearning analytics” — a set of techniques employing multiple sources of data (video, logs, text, artifacts,audio, gestures, biosensors) to examine learning in realistic, ecologically valid, social, mixed-medialearning environments.The incorporation of multimodal techniques, which are extensively used in the multimodal interactioncommunity, should enable researchers to examine unscripted, complex tasks in more holistic ways. Inparticular, we have focused on describing a set of modalities that have been the topic of multimodalanalysis for decades, as well as modalities that have recently emerged as new data streams throughwhich researchers can study human interaction and behaviour.REFERENCESAlvarado, C., & Davis, R. (2006). Dynamically constructed Bayes nets for multi-domain sketchunderstanding. Proceedings of the ACM SIGGRAPH 2006 Courses (A. 33).http://dx.doi.org/10.1145/1281500.1281544Alvarado, C., Oltmans, M., & Davis, R. (2002). A framework for multi-domain sketch recognition.Proceedings of AAAI Spring Symposium on Sketch Understanding, Palo Alto, California (pp. 1–8).Retrieved from http://rationale.csail.mit.edu/publications/Alvarado2002Framework.pdfAnthony, L., Yang, J., Koedinger, K. (2007). Adapting handwriting recognition for applications in algebralearning. Proceedings of the ACM Workshop on Educational Multimedia and MultimediaEducation (EMME 2007), 47–56. http://dx.doi.org/10.1145/1290144.1290153(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 234Baker, R., D’Mello, S. K., Rodrigo, M. M. T., Graesser, A. C. (2010). Better to be frustrated than bored:The incidence, persistence, and impact of learners’ cognitive-affective states during interactionswith three different computer-based learning environments. International Journal of Human–Computer Studies, 68(4), 223–241. http://dx.doi.org/10.1016/j.ijhcs.2009.12.003Barron, B., & Darling-Hammond, L. (2010). Prospects and challenges for inquiry-based approaches tolearning. In H. Dumont, D. Istance, & F. Benavides (Eds.), The nature of learning: Using researchto inspire practice (pp. 199-225). Paris: OECD.Beck, J. E., & Sison, J. (2006). Using knowledge tracing in a noisy environment to measure studentreading proficiencies. International Journal of Artificial Intelligence in Education, 16(2), 129–143.Burt, K. B., & Obradović, J. (2013). The construct of psychophysiological reactivity: Statistical andpsychometric issues. Developmental Review, 33(1), 29–57.http://dx.doi.org/10.1016/j.dr.2012.10.002Chang, K. M., Nelson, J., Pant, U., & Mostow, J. (2013). Toward exploiting EEG input in a readingtutor. International Journal of Artificial Intelligence in Education, 22(1), 19–38.http://dx.doi.org/10.1007/978-3-642-21869-9_31Chang, M., & Forbus, K. (2012). Using quantitative information to improve analogical matching betweensketches. In Proceedings of the 24th Annual Conference on Innovative Applications of ArtificialIntelligence (AAAI-12), Toronto, Canada, 2269–2274.Choi, M. H. C., Su-Jeong Lee, S. J. L., Jae-Woong Yang, J. W. Y., Ji-Hye Kim, J. H. K., Jin-Seung Choi, J. S. C.,Jang-Yeon Park, J. Y. P., ... & Dae-Woon Lim, D. W. L. (2010). Changes in cognitive performancedue to three types of emotional tension. International Journal of Bio-Science and Bio-Technology, 2(4), 23–28. http://dx.doi.org/10.1007/978-3-642-17622-7_26Conati, C., & Maclaren, H. (2009). Empirically building and evaluating a probabilistic model of user affect.User Modeling and User-Adapted Interaction, 19(3), 267–303.http://dx.doi.org/10.1007/s11257-009-9062-8Cowley, B., Ravaja, N., & Heikura, T. (2013). Cardiovascular physiology predicts learning effects in aserious game activity. Computers & Education, 60(1), 299–309.http://dx.doi.org/10.1016/j.compedu.2012.07.014Craig, S. D., D’Mello, S., Witherspoon, A., & Graesser, A. (2008). Emote aloud during learning withAutoTutor: Applying the facial action coding system to cognitive-affective states during learning.Cognition & Emotion, 22(5), 777–788. http://dx.doi.org/10.1080/02699930701516759D’Mello, S., Craig, S., Witherspoon, A., McDaniel, B., & Graesser, A. (2008). Automatic detection oflearner’s affect from conversational cues. User Modeling and User-Adapted Interaction, 18(1),45–80.http://dx.doi.org/10.1007/s11257-007-9037-6de Koning, B. B., Tabbers, H. K., Rikers, R. M., & Paas, F. (2010). Attention guidance in learning from acomplex animation: Seeing is understanding? Learning and Instruction, 20(2), 111–122.http://dx.doi.org/10.1016/j.learninstruc.2009.02.010Dewey, J. (1902). The school and society. Chicago, Il: University of Chicago Press.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 235Dym, C. L. (1999). Learning engineering: Design, languages, and experiences. Journal of EngineeringEducation, 88(2), 145–148. http://dx.doi.org/10.1002/j.2168-9830.1999.tb00425.xForbes-Riley, K., Rotaru, M., & Litman, J. (2009). The relative impact of student affect on performancemodels in a spoken dialogue tutoring system. User Modeling and User-Adapted Interaction(Special Issue on Affective Modeling and Adaptation), 18(1–2), 11–43.http://dx.doi.org/10.1007/s11257-007-9038-5Forbus, K., Usher, J., Lovett, A., Lockwood, K., & Wetzel, J. (2011). CogSketch: Sketch understanding forcognitive science research and for education. Topics in Cognitive Science, 3(4), 648–666.http://dx.doi.org/10.1111/j.1756-8765.2011.01149.xFreire, P. (1970). Pedagogia do Oprimido. Rio de Janeiro: Paz e Terra.Gomes, J. S., Yassine, M., Worsley, M., & Blikstein, P. (2013). Analysing engineering expertise of highschool students using eye tracking and multimodal learning analytics. In S. K. DʼMello, R. A.Calvo, & A. Olney (Eds.), Proceedings of the 6th International Conference on Educational DataMining (EDM 2013), (pp. 375–377). Retrieved fromhttp://www.educationaldatamining.org/EDM2013/papers/rn_paper_88.pdfHowison, M., Trninic, D., Reinholz, D., & Abrahamson, D. (2011). The Mathematical Imagery Trainer:from embodied interaction to conceptual learning. In G. Fitzpatrick, C. Gutwin, B. Begole, W. A.Kellogg & D. Tan (Eds.), Proceedings of the annual meeting of CHI: ACM Conference on HumanFactors in Computing Systems (CHI 2011), (Vol. \"Full Papers\", pp. 1989-1998).http://dx.doi.org/10.1145/1978942.1979230Hussain, M. S., AlZoubi, O., Calvo, R. A., & D’Mello, S. K. (2011). Affect detection from multichannelphysiology during learning sessions with AutoTutor. In G. Biswas, S. Bull, J. Kay, A. Mitrovic(Eds.), Proceedings of the 15th International Conference on Artificial Intelligence in Education(AIED), (pp. 131–138). http://dx.doi.org/10.1007/978-3-642-21869-9_19Jee, B., Gentner, D., Forbus, K., Sageman, B., & Uttal, D. (2009). Drawing on experience: Use of sketchingto evaluate knowledge of spatial scientific concepts. In N. Taatgen & H. van Rijn (Eds.),Proceedings of the 31st Annual Conference of the Cognitive Science Society (CogSci 2009), (pp.2499–2504). Retrieved from http://blog.silccenter.org/publications_pdfs/jee etal_CogSci2009_Final.pdfJermann, P., Mullins, D., Nüssli, M.-A., & Dillenbourg, P. (2011). Collaborative gaze footprints: Correlatesof interaction quality. In H. Spada, G. Stahl, N. Miyake, N. Law (Eds.), Proceedings of the 11thInternational Conference on Computer-Supported Collaborative Learning (CSCL 2011), (Vol.1, pp.184–191). Hong Kong: International Society of the Learning Sciences.Kirschner, P. A., Sweller, J., & Clark, R. E. (2006). Why minimal guidance during instruction does notwork: An analysis of the failure of constructivist, discovery, problem-based, experiential, andinquiry-based teaching. Educational Psychologist, 41(2), 75–86.http://dx.doi.org/10.1207/s15326985ep4102_1Klahr, D., & Nigam, M. (2004). The equivalence of learning paths in early science instruction.Psychological Science, 15(10), 661. http://dx.doi.org/10.1111/j.0956-7976.2004.00737.x(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 236Levy, F., & Murnane, R. J. (2004). The new division of labor: How computers are creating the next jobmarket. Princeton, NJ: Princeton University Press.Litman, D., Moore, J., Dzikovska, M., & Farrow, E. (2009). Using natural language processing to analyzetutorial dialogue corpora across domains and modalities. Proceedings of the 2009 conference onArtificial Intelligence in Education: Building Learning Systems that Care: From KnowledgeRepresentation to Affective Modelling (AIED ʼ09), (pp. 149–156). The Netherlands: IOS Press.Retrieved from http://people.cs.pitt.edu/~litman/paper_125.pdfLuft, C. D., Nolte, G., & Bhattacharya, J. (2013). High-learners present larger mid-frontal theta power andconnectivity in response to incorrect performance feedback. The Journal of Neuroscience, 33(5),2029–2038. http://dx.doi.org/10.1523/JNEUROSCI.2565-12.2013Lunn, D., & Harper, S. (2010). Using galvanic skin response measures to identify areas of frustration forolder web 2.0 users. Proceedings of the 2010 International Cross Disciplinary Conference on WebAccessibility (W4A ’10), (p. 34). http://dx.doi.org/10.1145/1805986.1806032Mason, L., Pluchino, P., Tornatora, M. C., & Ariasi, N. (2013). An eye-tracking study of learning fromscience text with concrete and abstract illustrations. The Journal of Experimental Education,81(3), 356–384.http://dx.doi.org/10.1080/00220973.2012.727885Montessori, M. (1965). Spontaneous activity in education. New York: Schocken Books.Muldner, K., Burleson, W., & VanLehn, K. (2010). “Yes!” Using tutor and sensor data to predict momentsof delight during instructional activities. In P. de Bra, A. Kobsa, D. Chin (Eds.), Proceedings of18th International Conference, UMAP 2010: User modeling, adaptation, and personalization(Lecture Notes in Computer Science Series), (Vol. 6075, pp. 159–170).http://dx.doi.org/10.1007/978-3-642-13470-8_16Munteanu, C., Penn, G., & Zhu, X. (2009). Improving automatic speech recognition for lectures throughtransformation-based rules learned from minimal data. Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP (Vol. 2, Vol. 2), (pp.764–772). Stroudsburg, PA: ACM.Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., & Ng, A. Y. (2011). Multimodal deep learning. Proceedingsof the 28th International Conference on Machine Learning (ICML-11), (pp. 689–696). Retrieved from http://machinelearning.wustl.edu/mlpapers/papers/ICML2011Ngiam_399Ocumpaugh, J., Baker, R. S. J. d., Rodrigo, M. M. T. (2012). Baker-Rodrigo Observation Method Protocol(BROMP) 1.0: Training Manual. Version 1.0. [Technical Report]. New York: EdLab. Manila,Philippines: Ateneo Laboratory for the Learning Sciences.Papert, S. (1980). Mindstorms: Children, computers, and powerful ideas. New York: Basic Books.Pardos, Z. A., Baker, .R. S. J. d., San Pedro, M. O. C. Z., Gowda, S. M., Gowda, S. M. (2013). Affectivestates and state tests: Investigating how affect throughout the school year predicts end of yearlearning outcomes. Proceedings of the 3rd International Conference on Learning Analytics andKnowledge (LAK ’13), 117–124. http://dx.doi.org/10.1145/2460296.2460320(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 237Raca, M., Tormey, R., & Dillenbourg, P. (2014). Sleepers’ lag-study on motion and attention. Proceedingsof the 4th International Conference on Learning Analytics and Knowledge (LAK ʼ14), 36–43.http://dx.doi.org/10.1145/2567574.2567581Read, J. C. (2007). A study of the usability of handwriting recognition for text entry by children.Interacting with Computers, 19(1), 57–69. http://dx.doi.org/10.1007/1-84628-062-1_9Richardson, D. C., & Dale, R. (2005). Looking to understand: The coupling between speakers’ andlisteners’ eye movements and its relationship to discourse comprehension. Cognitive science,29(6), 1045–1060. http://dx.doi.org/10.1207/s15516709cog0000_29Russ, R. S., Lee, V. R., & Sherin, B. L. (2012). Framing in cognitive clinical interviews about intuitivescience knowledge: Dynamic student understandings of the discourse interaction. ScienceEducation, 96(4), 573–599. http://dx.doi.org/10.1002/sce.21014Scherer, S., Glodek, M., Layher, G., Schels, M., Schmidt, M., Brosch, T., ... & Palm, G. (2012). A genericframework for the inference of user states in human computer interaction. Journal onMultimodal User Interfaces, 6(3–4), 117–141.Schick, A. Morlock, D., Amma, C., Schultz, T., & Stiefelhagen, R. (2012). Vision-based handwritingrecognition for unrestricted text input in mid-air. Proceedings of the 14th ACM InternationalConference on Multimodal Interaction (ICMI ’12), 217–220. New York: ACM.http://dx.doi.org/10.1145/2388676.2388719Schlömer, T., Poppinga, B., Henze, N., & Boll, S. (2008). Gesture recognition with a Wii controller.Proceedings of the 2nd International Conference on Tangible and Embedded Interaction (TEI ’08),11–14. http://dx.doi.org/10.1145/1347390.1347395Schneider, B., & Blikstein, P. (2014). Unraveling Students’ Interaction Around a Tangible Interface usingGesture Recognition. In J. Stamper, Z. Pardos, M. Mavrikis, B. Mclauren (Eds.), Proceedings ofthe 7th International Conference on Educational Data Mining (EDM’14), (pp.320-323). Retrievedfrom http://educationaldatamining.org/EDM2014/proceedings/EDM2014Proceedings.pdfSchneider, B., & Pea, R. (2013). Real-time mutual gaze perception enhances collaborative learning andcollaboration quality. International Journal of Computer-Supported Collaborative Learning, 8(4),375–397. http://dx.doi.org/10.1007/s11412-013-9181-4Schneider, B., & Pea, R. (2014). Toward Collaboration Sensing. International Journal of Computer-Supported Collaborative learning, 9(4), 371-395. http://dx.doi.org/10.1007/s11412-014-9202-ySchneider, B., Sharma, K., Cuendet, S., Zufferey, G., Dillenbourg, P., & Pea, A. D. (2015). 3D tangiblesfacilitate joint visual attention in dyads. In O. Lindwall, P. Hakkinen, T. Koschmann, P.Tschounikine, S. Ludvigsen (Eds.), In Proceedings of the International Conference on ComputerSupported Collaborative Learning 2015: Exploring the Material Conditions of Learning (CSCL’15),(Vol.1, pp. 158–165). Gothenburg, Sweden: The International Society of the Learning Sciences.She, H. C., Jung, T. P., Chou, W. C., Huang, L. Y., Wang, C. Y., & Lin, G. Y. (2012). EEG dynamics reflect thedistinct cognitive process of optic problem solving. PLOS ONE, 7(7), e40731.http://dx.doi.org/10.1371/journal.pone.0040731(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 238Sherin, B. (2013). A computational study of commonsense science: An exploration in the automatedanalysis of clinical interview data. Journal of the Learning Sciences, 22(4), 600–638.http://dx.doi.org/10.1080/10508406.2013.836654Song, Y., Morency, L. P., & Davis, R. (2012). Multimodal human behavior analysis: Learning correlationand interaction across modalities. Proceedings of the 14th ACM International Conference onMultimodal Interaction (ICMI ’12), 27–30. http://dx.doi.org/10.1145/2388676.2388684Stevens, R. H., Galloway, T., & Berka, C. (2007). EEG-related changes in cognitive workload, engagementand distraction as students acquire problem solving skills. In C. Conati, K. McCoy, G. Paliouras(Eds.), Proceedings of the 11th International Conference on User Modeling, (pp. 187–196). BerlinHeidelberg: Springer Link. http://dx.doi.org/10.1007/978-3-540-73078-1_22Weinland, D., Ronfard, R., & Boyer, E. (2006). Free viewpoint action recognition using motion historyvolumes. Computer Vision and Image Understanding, 104(2), 249–257.http://dx.doi.org/10.1016/j.cviu.2006.07.013Worsley, M. (2014). Multimodal learning analytics as a tool for bridging learning theory and complexlearning behaviors. Proceedings of the 2014 ACM workshop on Multimodal Learning AnalyticsWorkshop and Grand Challenge (MLA ’14), 1–4. http://dx.doi.org/10.1145/2666633.2666634Worsley, M., & Blikstein, P. (2011). What’s an expert? Using learning analytics to identify emergentmarkers of expertise through automated speech, sentiment and sketch analysis. In M.Pechenizkiy, T. Calders, C. Conati, S. Ventura, C. Romero, & J. Stamper (Eds.), Proceedings of the4th Annual Conference on Educational Data Mining (EDM 2011), (pp. 235–240).Worsley, M., & Blikstein, P. (2013). Toward the development of multimodal action based assessment.Proceedings of the 3rd International Conference on Learning Analytics and Knowledge (LAK ’13),94–101. http://dx.doi.org/10.1145/2460296.2460315Worsley, M., & Blikstein, P. (2014). Deciphering the practices and affordances of different reasoningstrategies through multimodal learning analytics. Proceedings of the 2014 ACM workshop onMultimodal Learning Analytics Workshop and Grand Challenge (MLA ’14), 21–27.http://dx.doi.org/10.1145/2666633.2666637Worsley, M., & Blikstein, P. (2015). Using learning analytics to study cognitive disequilibrium in acomplex learning environment. Proceedings of the 5th International Conference on LearningAnalytics and Knowledge (LAK ʼ15), 426–427. http://dx.doi.org/10.1145/2723576.2723659Yilmaz, A., & Shah, M. (2005). Actions sketch: A novel action representation. In C. Schmid, S. Soatto, C.Tomasi (Eds.), Proceedings of the 2005 IEEE Computer Society Conference on Computer Visionand Pattern Recognition (CVPR 2005), (Vol. 1, pp. 984–989). Los Alamitos, CA: IEEE ComputerSociety. http://dx.doi.org/10.1109/CVPR.2005.58Zaïane, O. R. (2001). Web usage mining for a better web-based learning environment. In Proceedings ofthe Conference on Computers and Advanced Technology in Education (CATE 2001), (pp. 60–64).https://webdocs.cs.ualberta.ca/~zaiane/postscript/CATE2001.pdf"  )

  const splitter = new TokenTextSplitter({
    encodingName: 'gpt2',
    chunkSize: 200,
    chunkOverlap: 0,
  })
  console.log(await splitter.splitText(documentContent))


  const vectorStore = new MemoryVectorStore(embeddings)
  await vectorStore.addVectors([[1,2,3,3,4], [2,2,2,2,2]], [new Document({pageContent: 'awawd dawdawd' }),
    new Document({pageContent: "awdawdaw"})])
  console.log("ss")
  console.log(vectorStore)
  const retriver = await vectorStore.asRetriever(1)
  console.log(await retriver.getRelevantDocuments("wwwww"))

  // MemoryVectorStore.fromDocuments(res, embeddings).then(vectorStore =>{
  //   vectorStore.addDocuments(res).then(() => {
  //     const vectorStoreRetriever = vectorStore.asRetriever(3);
  //     vectorStoreRetriever.getRelevantDocuments(
  //       "what did he say about ketanji brown jackson"
  //     ).then(resDoc => {
  //       console.log(resDoc);
  //       console.log(vectorStore)
  //       // Browser.storage.local.set({ 'vectorStorage': vectorStore })
  //     })
  //   })
  //
  // });
  // const splitedDocStr = res.map((val) => {
  //   return val.pageContent
  // })
  // embeddings.embedDocuments(splitedDocStr).then((embeddingDoc) => {
  //   console.log(embeddingDoc)
  //   embeddings.embedQuery("wdawd dawg awgaw").then(res => {
  //     console.log(res)
  //   })
  // })
}
export const splitText = async (fileText, config) => {
  const splitter = new TokenTextSplitter({
    encodingName: 'gpt2',
    chunkSize: config.chunkSize,
    chunkOverlap: 0,
  })
  console.log("chunkSize", config.chunkSize)
  console.log(fileText)
  const returnList = []
  let res = await splitter.splitText(fileText)
  console.log("initial res ↓")
  console.log(res)
  while (res.length > 1) {
    returnList.push(...res.slice(0, res.length - 1))
    res = await splitter.splitText(res[res.length - 1])
  }
  returnList.push(...res)
  console.log(returnList)
  return returnList
}
export const test = async () => {
  const fileText = "(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 220Multimodal Learning Analytics and Education Data Mining: UsingComputational Technologies to Measure Complex Learning TasksPaulo BliksteinGraduate School of Education and (by courtesy) Computer Science DepartmentStanford University, USApaulob@stanford.eduMarcelo WorsleyLearning Sciences & Computer ScienceNorthwestern University, USAmarcelo.worsley@northwestern.eduABSTRACT: New high-frequency multimodal data collection technologies and machine learninganalysis techniques could offer new insights into learning, especially when students have theopportunity to generate unique, personalized artifacts, such as computer programs, robots, andsolutions engineering challenges. To date most of the work on learning analytics and educationaldata mining has been focused on online courses and cognitive tutors, both of which provide ahigh degree of structure to the tasks, and are restricted to interactions that occur in front of acomputer screen. In this paper, we argue that multimodal learning analytics can offer newinsights into student learning trajectories in more complex and open-ended learningenvironments. We present several examples of this work and its educational applications.Keywords: Learning analytics, multimodal interaction, constructivism, constructionism,assessment1 INTRODUCTIONThe same battle is fought in every field of educational research and practice: the champions of the directinstruction of well-defined content pitted against those who encourage student-centred exploration ofill-defined domains. These wars have taken place repeatedly over past decades, and partisans on eachside have been reborn in multiple incarnations. The first tradition tends to be aligned with behaviouristor neo-behaviourist approaches, while the second favours constructivist-inspired pedagogies. Inlanguage arts, the battle has been between phonics and the whole word approach. In math, war iswagged between teaching algorithms versus instruction in how to think mathematically. In history, theyclash over the relative merits of critical interpretations and the memorization of historical facts. Inscience, they clash about inquiry-based approaches versus direct instruction of formulas and principles.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 221The educational research community has always maintained that the debate would end when researchresults inevitably demonstrated the superiority of one of the sides. Yet this conclusion has eludedscholarship for decades. One of the reasons for this interminable contest is that the underlying rationalefor the differences concerns individual values and societal beliefs and will not be resolved by a purelyscientific approach. In fact, the whole debate may serve the educational research community in quite adifferent capacity. More specifically the debates may reveal the underlying visions for what educationshould be about, for different groups, and we might more profitably re-examine the nature and purposeof our schools. More fundamentally, is education a tool for filtering, ranking, emancipation, socialequalization, economic progress, meritocracy, or for the promotion of social Darwinism?Educational scholars would greatly differ in their answers — rendering the question of “which approachis better,” and “what evidence suffices,” pointless. As with the debates on public healthcare and fiscalpolicy, despite our best efforts to generate reliable research, the “best” way to conduct education willalways be controversial and dependent on larger societal and political winds. But the fundamentalproblem, and the motivation for this article, is that the prevailing issue is not who “wins” the debate, butrather the existence of a healthy debate. Fostering a healthy debate requires some level of symmetry.However, as it stands, the playing field is not symmetrical. The “direct instruction” approach isinherently easier to test and quantify using currently available tools that include mass-production ofcontent and decades of research concerning psychometrics and standardized testing strategies.Meanwhile, the constructivist side counts on laborious interventions, and complex mixed-moderesearch methods. The result of this asymmetry is that public systems, more dependent on high-profileresearch results, are left, by inertia, to the designs of the proponents of traditional approaches, whileonly affluent schools, private or public, who can experiment more, can afford to implement modern,constructivist approaches to learning.Learning analytics could deepen this asymmetry, or help eliminate it. The elimination of the asymmetrycould re-establish a healthy public debate around education, where both sides would have comparableand credible results to show, and policy makers would be able to make choices based on their valuesand visions for education. However, the deepening of this asymmetry could be a significant impedimentto progressive education and the vision of creating alternative learning environments that can reach amore diverse population of learners. Should public education succumb to the temptation of the fiscalbenefits supposedly offered by total automatization and its much lower baseline for cost and quality, allother options would be driven into the ground as economically unfeasible: who could compete withvirtually free computerized tutors and videos? How many years would the debate take, while childrencaught in the “experimental” years are being victimized?Consequently, we propose that an important goal of learning analytics is to equalize the playing field bydeveloping methods that examine and quantify non-standardized forms of learning. We suggest thatthis need for a level playing field is more necessary than ever, given the increasing demand for scalableproject-based, interest-driven learning and student-centred pedagogies (e.g., Papert, 1980). Within our(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 222increasingly interconnected societal and economic environment — which has become pervaded bytechnology and threatened by challenging global problems such as climate change — both K–12 anduniversity-level engineering education (Dym, 1999) demand higher-level, complex problem-solving asopposed to performance in routine cognitive tasks (Levy & Murnane, 2004). Approaches that placepremiums on student-centred, constructivist, self-motivated, self-directed learning have beenadvocated for decades (e.g., Dewey, 1902; Freire, 1970; Montessori, 1965; Barron & Darling-Hammond,2010) but have failed to become scalable and prevalent, and have come under attack during the lastdecade (e.g., Kirschner, Sweller, & Clark, 2006; Klahr & Nigam, 2004).New high-frequency data collection technologies and machine learning could offer new insights intolearning in tasks in which students are allowed to generate unique, personalized artifacts, such ascomputer programs, robots, movies, animations, and solutions to engineering challenges. To date mostof the work on learning analytics and educational data mining has focused on tasks that are computer-mediated and are more structured and scripted. In this work, we argue that multimodal data collectionand analysis techniques (“multimodal learning analytics” or MMLA) could yield novel methods thatgenerate distinctive insights into what happens when students create unique solution paths toproblems, interact with peers, and act in both the physical and digital worlds.Assessment and feedback is particularly difficult within these open-ended environments, and theselimitations have hampered many attempts to make such approaches more prevalent. Automated, fine-grained data collection and analysis could help resolve this tension in two ways. First, such capacitieswould give researchers tools to examine student-centred learning in unprecedented scale and detail.Second, these techniques could improve the scalability of these pedagogies since they make feasibleboth assessment and formative feedback, which are typically very complex and laborious in suchenvironments. They might not only reveal students’ trajectories throughout specific learning activities,but they could also help researchers design better supports, pedagogical approaches, and learningmaterials.At the same time, in the well-established field of multimodal interaction, new data collection andsensing technologies are making it possible to capture massive amounts of data in all fields of humanactivity. These technologies include the logging of computer activities, wearable cameras, wearablesensors, biosensors (e.g., that permit measurements of skin conductivity, heartbeat, andelectroencephalography), gesture sensing, infrared imaging, and eye tracking. Such techniques enableresearchers to have unprecedented insight into the minute-by-minute development of a number ofactivities, especially those involving multiple dimensions of activity and social interaction. However, thetechnologies just mentioned have not yet become popular in the field of learning analytics. We proposethat multimodal learning analytics could bring together these multiple techniques in morecomprehensive evaluations of complex cognitive abilities, especially in environments where theprocesses or outcomes are unscripted. Thus, the goal of this paper is to demonstrate the feasibility andpower of novel assessment techniques in several modalities and learning contexts.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 2232 STATE OF THE FIELDIn considering the current sensing and assessment modalities possible using MMLA, we see three non-mutually exclusive areas: assessing student knowledge, assessing student affect and physiology, andassessing student intentions or beliefs. At the crux of all these forms of student characterization is theunderlying invocation of data analysis to generate useful models from large sets of quantitative data.Hence, what varies in the different forms of student assessment is the source of the raw data and howthat data is translated into computable data. Once the translation has been completed, the data isprocessed using a collection of machine learning algorithms. In what follows, we present severalmethods being used to capture and process student data. There are several techniques — web datamining, user data mining, simple web-based surveys, etc. — but the following technologies have beenselected for inclusion because they live on the cutting-edge of technology and help promote the notionof “natural” assessment (Zaïane, 2001). Furthermore, while each of these technologies represents aresearch contribution in and of itself, our interest in including them is to bring to the forefront a widervariety of non-traditional approaches that education researchers and educational data scientists canbegin to combine in their learning analytics research. For the first three techniques we mention — textanalysis, speech analysis, and handwriting analysis — our discussion will be very cursory, as theserepresent areas of research that have received considerable attention with the computer sciencecommunity, and have started to get traction within the learning analytics community. Nonetheless, wewant to make the reader aware of some of the current capabilities and research in these areas. For thelatter analyses that we discuss, we will engage in a more detailed and descriptive explanation of each, asthese domains remain relatively new, even among the computer science community.2.1 Text AnalysisWhile text analysis, or natural language processing, has been around for decades it is only in recenthistory that education has begun to benefit from this technology, and researchers have targetedlearners’ text explicitly. Despite the fact that text itself is not multimodal, text analysis allows for theinterpretation of open-ended writing tasks, differently from multiple-choice tests. Given that collectingtext from students is unproblematic both technically and logistically, it constitutes one of the mostpromising modalities for MMLA: text can be easily gathered from face-to-face and online activities, fromtests and exams, and from expert-generated prose from textbooks and online sources (often used asbaseline). For example, Sherin (2013) has been doing pioneering work in the analysis of text in thelearning sciences community. He uses techniques from topic modelling and clustering to study theprogression of students’ ideas and intuitions as they describe the explanation for the existence of thefour seasons (Sherin, 2013). More specifically, he shows that, as students explain the seasons, invokingdifferent types of scientific explanations, it is possible to identify which type of explanation each studentis referring to at different points in time. He also goes beyond this to show how students can beaccurately clustered without using a pre-defined set of exemplar responses, but instead by usingautomatically derived topics models from the corpus itself. This approach of clustering segments of(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 224students’ text based on the descriptions of their peers is a powerful tool that can allow researchers andpractitioners to draw meaningful commonalities and differences among large populations of students,without having to explicitly read and compare the entirety of each transcript. Given the prevalence oftext-based assessment and the intensive use of text in face-to-face and online learning, this promisingmethod will likely accelerate discourse-based research, and open new possibilities for large-scaleanalysis of open-ended text corpora.2.2 Speech AnalysisSpeech analysis shares many of same goals and tools as text analysis. Speech analysis, however, furtherremoves the student from the traditional assessment setting by allowing them to demonstrate fluencyin a more natural setting. For example, Worsley & Blikstein (2011) studied how elements of studentspeech, as inferred by linguistic, textual, and prosodic features, can be predictive for identifyingstudents’ level of expertise on open-ended engineering design tasks. In addition to traditional linguisticand prosodic features, speech signals can be analyzed for a wealth of other characteristics. Variousresearch tools have been developed to help researchers in the process of extracting these features,however, several challenges remain in knowing how to analyze student learning appropriately using saidfeatures.Other researchers have moved away from raw analysis of the speech signal to leverage speechrecognition capabilities. In particular, Beck and Sison (2006) demonstrated a method for using speechrecognition to assess reading proficiency. As an extension of Project LISTEN — an intelligent tutor thathelps elementary school students improve their reading skills — researchers completed a study thatcombines speech recognition with knowledge tracing, a form of probabilistic monitoring. By having alanguage model that was largely restricted to the content of each book being learned, the work requiredfor doing automatic speech recognition, and subsequent accuracy classification, was greatly simplified.Outside of the education domain there have been decades of work in developing speech recognizersand dialogue managers. However, to date, such technologies are still not widely used in educationbecause of the challenges associated with building a satisfactory language model that can reliablyrecognize speech. Munteanu, Peng, and Zhu (2009) have made some progress in this area by showinghow to improve speech recognition of lectures in college-level STEM class. A primary consideration inthe area of speech recognition, therefore, will be to identify the most effective ways to use thistechnology in real-world educational settings. Although using it to transcribe lectures might be feasible,the challenge of collecting and interpreting student data seems extremely difficult. Differently fromother applications of speech recognition (smartphones, personal assistants, dictation), educationalapplications need to address simultaneously classroom noise, multiple overlapping speakers, andlogistical difficulties in voice training — very ambitious challenges that have not been solved yet.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 2252.3 Handwriting AnalysisA different form of text analysis is handwriting analysis, which is important in educational settingsbecause a considerable part of the work done by students is still handwritten. Anthony, Yang, andKoedinger (2007) highlight the affordances of combining handwriting recognition with intelligent tutorsfor algebra. Based on their study of high school and middle school students, introducing handwritingrecognition halved the time students needed to complete tutoring activities because students no longerhad to deal with cumbersome keyboard and mouse-based entry. This is significant because it enablesstudents to focus on understanding the material using familiar forms of interaction as opposed tostruggling to learn a new interface. Accordingly, handwriting recognition can facilitate more effectivelearning by eliminating the barriers to using certain computer-based interfaces. It also permits thestudent to learn in such a way that more closely parallels the usual mathematics environment (i.e.,utilizing a writing tool as opposed to a keyboard), which may increase transfer.Researchers also studied the use of handwriting recognition technology among school-aged children(Read, 2007), examining the length and quality of stories produced by students using different inputmethods. A primary finding of this work was that students were more willing to engage in the writingprocess when using digital ink than when using traditional keyboard input. However, the team still foundthat handwriting recognition technology was not yet comparable to traditional paper and pencil.Similarly to Anthony et al. (2007), Read (2007) emphasizes the affordances of handwriting as a morenatural form of authorship that may help students better engage in learning.More recent work extends handwriting recognition to mid-air “writing” that achieves high levels ofaccuracy by utilizing a combination of computer vision, multiple cameras, and machine learning (Schick,Morlock, Amma, Schultz, & Stiefelhagen, 2012). This approach highlights some of the more recentopportunities in handwriting recognition in novel learning environments and contributes to thediscussion around the expansive possibilities available away from traditional keyboards and screens.2.4 Sketch AnalysisWhereas handwriting analysis is primarily concerned with looking for words, others researchers haveembarked on work that looks at both text-based and graphic-based representations. Fundamental workon object recognition and sketches is that of Alvarado, Oltmans, and Davis (2002) and Alvarado andDavis (2006). These researchers developed a framework for performing multi-domain recognition ofsketches using Bayesian networks and a predetermined set of shapes and patterns for each domain.With the predefined shapes and patterns, their algorithm is able to decipher messy sketches from thedomains of interest.Ken Forbus and colleagues also describe seminal work in the development of both systems andtechniques for analyzing and comparing sketches among learners. For example, Jee, Gentner, Forbus,(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 226Sageman, & Uttal (2009) explain the design and implementation of CogSketch, a tool used to study howstudents of different levels of experience describe common scientific concepts in geology throughsketches (Forbus, Usher, Lovett, Lockwood, & Wetzel, 2011). CogSketch pays particular attention to boththe content and the process of the sketches being developed. Chang and Forbus (2012) extend this workon qualitative sketching to include quantitative analysis of sketching, which allows them to garner amore accurate representation and understanding of the sketches.Sketching is particularly important given the current focus on conceptual learning in STEM. One of themost popular forms of eliciting student knowledge in science has been the creation of diagrams andconcept maps. From this prior work, it is apparent that a number of research groups have demonstratedthe ability to do meaningful analyses of sketches in order to study cognition and learning.2.5 Action and Gesture AnalysisAction recognition has recently received considerable attention within the computer vision community.For example, work by Weinland, Ronfard, & Boyer (2006) and Yilmaz and Shah (2005), among others,has demonstrated the ability to detect basic human actions related to movement. The work of Weinlandet al. (2006) involved developing a technique that could capture user actions independent of gender,body size, and viewpoint. The work of Yilmaz and Shah (2005) involved human action recognition usinguncalibrated moving cameras, which might prove useful for the dynamic nature of classrooms and/orlaboratories.This kind of work is currently being applied to classroom settings as well. Raca, Tormey, and Dillenbourg(2014), for instance, are pioneering ways of capturing student engagement and attention by conductingframe-by-frame analyses of videos taken from the teacher’s position. They show that students’ motionand level of attention can be estimated using computer vision, and that individuals with lower levels ofattention are slower to react than focused students. This line of work opens the door for new kinds offeedback loops for teachers, by providing not only real-time information about students but alsoaggregate measures of their levels of attention over time.Other work in the area of gesture recognition has leveraged infrared cameras and accelerometers thatare affixed to the research subject. Using infrared, one avoids some of the complications that may existwith camera geometry, lighting, and other forms of visual variance. Using this approach Schlömer,Poppinga, Henze, and Boll (2008) demonstrate the ability to construct a gesture recognition system bycapturing and processing accelerometer data from a Nintendo Wiimote. Their technique allows them toreliably capture gestures for squares, circles, rolling, the shape “Z,” etc.More recent work has taken advantages of the Microsoft Kinect sensor and simple infrared detectors aslow cost tools for capturing and studying human gestures. The Mathematical Imagery Trainer (Howison,Trninic, Reinholz, & Abrahamson, 2011) uses hand gestures captured by the Kinect sensor as a way forstudying student understanding of proportions. Students use their hands to indicate the relationship(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 227between two values, and benefit from visual feedback on the correctness of their hand placement. Thissystem also enables teachers to give students real-time, immediate feedback and change theirinstruction as they perceive students’ difficulties (and not only after the fact), and points to one of themain benefits of multimodal learning analytics. As an even more basic example, the Kinect sensor can beused to give teachers and students immediate feedback about the amount of gesticulation that they aredoing. Hence, without requiring a set of recommended actions, low-cost sensing of movement could beuseful in helping students and teachers be more aware of their own behaviours.Related to the measurement of student gesticulation, early work in this domain by Worsley and Blikstein(2013) involved a comparison of hand/wrist movement between experts and novices as they completedan engineering design task. In particular, the researchers used hand/wrist movement data from a Kinectsensor to examine the extent of two-handed action, and found that experts were much more likely toemploy two-handed actions than novices. These preliminary results aligned with theories associatedwith two-handed inter-hemispheric actions, and provided initial motivation for studying gestures incomplex learning environments.In a similar line of work, Schneider and Blikstein (2014) used a Kinect sensor to evaluate studentstrategies when interacting with a Tangible User Interface (TUI): their task was to learn about the humanhearing system by interacting with 3D-printed organs of the inner ear. Using clustering algorithms, theauthors found that students’ body postures fell into three prototypical positions (Figure 1): an active,semi-active, or passive state. The amount of time spent in the active state was significantly correlatedwith higher learning gains, and the time spent in the passive state was significantly correlated with lowerlearning gains. More interestingly, the number of transitions between those states was the strongestpredictor of learning. Those results suggest that successful students went through cycles of reflectionand action, which helped them gain a deeper understanding of the domain taught. This approach showsthe potential of using clustering methods on gestures data to find recurring behaviours associated withhigher learning gain.Figure 1: Using k-means on student body posture (Schneider & Blikstein, 2014). The first state (left)is active, with both hands on the table; the second (middle) is passive, with arms crossed; the third(right) is semi-active, with only one hand on the table.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 228As a whole, the advances in action and gesture recognition, and the introduction of low-cost, high-accuracy sensors is creating additional opportunities for action and gesture recognition to be included ineducation research.2.6 Affective State AnalysisStudying students’ affective states can often be challenging and hard to validate. However, severalstudies have demonstrated that identifying affect can be done consistently, and that affect is animportant marker in studying and understanding learning.2.6.1 Human Annotated Affective StatesBaker, D’Mello, Rodrigo, & Graesser (2010) and Pardos, Baker, San Pedro, Gowda, & Gowda (2013) areexamples of work using human annotated affective states. In Pardos et al. (2013), the researchers usedthe Baker-Rodrigo Observation Method Protocol (BROMP) (Ocumpaugh, Baker, & Rodrigo, 2012) tocorrelate student behaviour and affect while participating in cognitive tutoring activities withperformance on standardized tests. They found that the learning gains associated with certain affectivestates, namely boredom and confusion, are highly dependent on the level of scaffolding that the studentis receiving. This finding builds on prior work that studies affective state as students participate incognitive tutoring activities (e.g., Litman, Moore, Dzikovska, & Farrow, 2009; Forbes-Riley, Rotaru, &Litman, 2009).2.6.2 Automatically Annotated Affective StateOther work, using the Facial Action Coding System (FACS), has demonstrated that researchers canrecognize student affective state by simply observing their facial expressions. In the case of Craig,D’Mello, Witherspoon, and Graesser (2008), researchers were able to perceive boredom, stress, andconfusion by applying machine learning to video data of the student’s face throughout the tutoringexperience. Data was collected while students interacted with AutoTutor, an intelligent tutoring systemfor learning science. The technique that Craig et al. (2008) validated is a highly non-invasive mechanismfor realizing student sentiment, and can be coupled with computer vision technology to enablemachines to detect changes in emotional state or cognitive-affect automatically. Worsley and Blikstein(2015) utilize the Facial Action Coding System to compare two different experimental conditions. Morespecifically, the authors compared the frequency and rate of transitions among four automaticallyderived affective states that are conjectured to be important for learning. In particular, they were ableto show that the two experimental conditions expressed significantly different rates of confusion anddiffered in how frequently they transitioned from neutral to surprise, and from neutral to confusion.Being in, or transitioning to a confused expression was generally associated with good outcomes,whereas being more surprised, or transitioning to an expression of surprise was generally associatedwith less favourable outcomes.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 229Researchers have also used conversational cues to realize students’ emotional states. Similar to Craig etal. (2008), D’Mello, Craig, Witherspoon, McDaniel, & Graesser (2008) designed an application that coulduse spoken dialogue to recognize the states of boredom, frustration, flow, and confusion. Researcherswere able to resolve the validity of their findings through comparison to emote-aloud (a derivative oftalk-aloud where participants describe their emotions as they feel them) activities while studentsinteracted with AutoTutor.2.6.3 Physiological Markers of Affective StateMore recent work in this space was able to accurately predict the affective state, and the source of thechange in affective state for users as they interact with a computer-based tutoring system (Conati &MacLaren, 2009). In particular, the system was able to predict when students experienced joy, distress,and admiration effectively. In the past years, other researchers have expanded the detection of affectwithin educational contexts to leverage physiological markers (Hussain, AlZoubi, Calvo, & D’Mello, 2011;Chang, Nelson, Pant & Mostow, 2013).Especially when dealing with web-based and tutoring activities, identifying the intensity and the time-occurrence of the emotional state is an important clue to distinguish an affective learning process froma pleasant, but not learning-effective, computer-based activity. Seeking clarity on this distinction,Muldner, Burleson, and VanLehn (2010), used physiological (skin conductance sensor and pupildilatation), behavioural (speaking aloud protocol, posture in the chair, and mouse clicks) and task-related data to predicted moments of excitement associated to learning, referred to as a “yes!”moment. They found that the “yes!” moment was associated with more reasoning, effort, andinvestment in solving the task, suggesting that the intensity of this positive emotion after theachievement of a goal may be a predictor of increased learning.This same physiological approach is also useful to identify negative feelings and reactions, which in turnis associated with lower performance in cognitive tasks. An increase in physiological reactivity wasobserved by Lunn and Harper (2010) to be associated with a frustrating web-based activity. Moreover,Choi et al. (2010) demonstrated that tense emotions induced by an external stimulus have a negativeeffect on performance in a subsequent cognitive task.The various studies of student affect emphasize the potential for empowering educators throughstudent sentiment awareness. Using one, or more, of the modalities of speech, psychophysiologicalmarkers, and computer vision, researchers are able to better understand the relationship betweenaffect and learning, and at a much more detailed level.2.7 Neurophysiological MarkersThough briefly mentioned in the previous section, there is a growing cadre of researchers doing work onpsychophysiology, and its relationship to cognition and learning. Burt and Obradović (2013) provide an(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 230overview of this domain, while also pinpointing key areas for researchers to pay attention to when doingthis work. Other researchers, such as Stevens, Galloway, and Berka (2007), describe the IMMEX systemused to study the electroencephalograms (EEGs) of students as they participate in a computer-basedenvironment. In their work, they also present preliminary findings on the relationship between EEG andcognitive load, distraction, and engagement. One unexpected finding of the research was that even as astudent’s skill level increased, the workload remained the same. This unexpected result highlights one ofthe key affordances of these new multimodal modes of analysis: they may challenge researchers toquestion previously held assumptions or intuitions about student learning. The study of Stevens et al.(2007) is only one among a host of cutting-edge publications that examine cardiovascular physiology(Cowley, Ravaja, & Heikura, 2013), mid-frontal brain activity (Luft, Nolte, & Bhattacharya, 2013), andother connections between cognition and physiology (Burt & Obradović, 2013).Moreover, studies vary in the number of sensors used as well as in the types of analyses. Using a singlechannel portable EEG device, Chang, Nelson, Pant & Mostow (2013) were able to distinguish easy anddifficult sentences read by children and adults. In a more complex task, nine EEG channels were used toidentify differences from solutions created by students when solving a maze problem that requiredphysics concepts. Students with better solutions (reduced number of leans used) had higher thetapower in the frontal areas of the brain, which is related to mental effort, concentration, and attention(She et al., 2012). Neuroimaging techniques increased the comprehension about brain mechanismsinvolved in learning as well in learning disabilities. Understanding brain mechanisms required forcognitive processing and learning is important to either adapt learning methodologies to specific topicsor create interventions for students with specific needs.2.8 Eye Gaze AnalysisAnother area applicable to educational research is eye tracking and gaze analysis. While this technologyhas long been used within the field of research on consumer electronics and software usage, recentwork in a variety of learning environments has shown eye tracking can be useful for understandingstudent learning. One of the constructs more related to eye gaze is attention. For example, Gomes,Yassine, Worsley, and Blikstein (2013) captured eye-tracking data from high school students as theycompleted a collection of engineering design games. By using machine learning to cluster the studentsbased on their gaze patterns, the team identified that the highest performing students used very similarpatterns in where they looked, how longed they looked, and their level of systematicity.Data from eye tracking also helps to understand what kind of approaches are useful in helping studentsto enhance learning. Mason, Pluchino, Tornatora, and Ariasi (2013) demonstrated that using pictures ina scientific text is better than using only text. However, based in the number of fixations in the final partof the text, the authors conclude that using an abstract picture that represents the topic studied (physicsphenomena) appears to be more efficient, i.e., same performance but less cognitive load than using aconcrete illustration about the same topic,(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 231However, de Koning, Tabbers, Rikers, and Paas (2010) argue that looking at specific stimulus canrepresent the student’s shifting of attention to possible areas of interest, but does not always mean thatthey are learning. In their study, students looked longer and more often at an instructional animationwith cues compared to the same animation without cues, but the authors could not confirm that givingcues would reduce the student’s cognitive load or even increase conceptual understanding.Notwithstanding, the most promising use of eye-tracking technology in education has been to studysmall collaborative learning groups. The overall framework for this type of work is to synchronize twoeye-trackers and compute the number of times a particular group achieves joint visual attention (JVA).JVA has been studied extensively in a variety of disciplines (developmental psychology, communication,learning sciences) and is known as a strong predictor of a group’s quality of collaboration. Richardsonand Dale (2005), for instance, found that the degree of gaze recurrence between individual speaker–listener dyads (i.e., the proportion of alignment of their gazes) was correlated with the listeners’accuracy on comprehension questions. In a remote collaboration, Jermann, Mullins, Nüssli, andDillenbourg (2011) describe how “good” programmers tend to have a higher recurrence of joint visualattention when having productive interactions, compared to less proficient programmers. Additionally,recent work by Schneider and Pea (2013) suggests that JVA is not just a proxy for predictingcollaboration, but can also be influenced to improve communication between students. They designedan intervention in which students worked in pairs (in different rooms). In one condition, the twoparticipants could see each other’s gaze; in the other condition, no such augmentation was provided.Their task was to study a set of diagrams to learn about the human visual system. Those who could seethe gaze of their partner in real time on the screen achieved significantly higher learning gains and had ahigher quality of collaboration. Those findings highlight the potential of using gaze-awareness tools foraugmenting student interactions in various learning environments and settings. It should be noted thatthose examples are limited to remote collaborations. Schneider et al., (2015) extends this line of work toco-located settings. Using mobile eye-trackers and computer vision algorithms, they were able toreplicate the findings above: in a side-by-side collaboration, JVA was found to be a significant predictorof student learning gains and performance on a problem-solving task.Finally, Schneider and Pea (2014) are expanding what can be predicted when combining JVA, networkanalysis and machine learning. In this work, they describe networks where nodes represent visualfixations and edges represent saccades. Their findings suggest that when those networks characterize adyad (i.e., the size of a node represents the amount of joint visual attention on one particular area of thescreen), different properties of the network are associated with different facets of a good collaboration.For instance, the extent to which students reach consensus during a problem-solving task is associatedwith the average size of the strongly connected components of the graphs. They found that otherdimensions of a productive collaboration (sustaining mutual understanding, dialogue management,information pooling, reaching consensus, task division, task management, technical coordination,(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 232reciprocal interaction, individual task orientation) could similarly be predicted by applying machine-learning algorithms on the features of those graphs.These studies suggest interesting opportunities to understand and enhance collaborative learning usingeye-tracking data. More specifically, they provide new ways to study small-group visual coordinationand its relationship to productive learning strategies. Recent work is generalizing this line of inquiryacross various settings, which opens promising new doors for predicting and influencing collaborationamong students.2.9 Multimodal Integration and Multimodal InterfacesHaving considered several example modalities currently being used by researchers to study studentlearning individually, we now turn to a final example that entails analysis using multiple modalities. Aspreviously noted, Multimodal Learning Analytics also builds on the idea of multimodal integration andmultimodal interfaces. Multimodal integration is the synchronous alignment and combination of datafrom different modalities (or contexts) in order to get a clearer understanding of the learning cues thatstudents are producing. Worsley (2014) and Worsley and Blikstein (2014) discuss and employ variousmultimodal learning analytics techniques. Worsley (2014) considers the impact of using differentmultimodal data fusion approaches. Specifically, the paper highlights naïve fusion, low-level (or data-level) fusion and high-level (or quasi feature-level) fusion as having differing levels of utility, and as beingassociated with different underlying research questions. Naïve fusion was the label given to multimodalanalyses that built machine-learning classifiers from the summary statistic generated from each of thedata streams or features. In many cases, these features are first subjected to feature selection in orderto reduce the feature space down to something reasonable. Low-level fusion (or feature fusion) involvedsynchronizing the data at each time step and conducting analyses on the features after they have beenfused together. Finally, high-level fusion is described as extracting one of more semantic level featuresfrom one or more data streams before fusing them with the other data streams. An example of thiswould be to do gesture recognition or speech recognition before aligning the hand/wrist movementand/or audio channels with the other data sources available for analysis.In Worsley and Blikstein (2014), the authors present a multimodal comparison from a two-conditionexperiment, in which students worked in pairs to complete an engineering design challenge. By usinghand/wrist movement, electro-dermal activation, and voice activity detection, the authors were able toidentify a set of representative multimodal states that students used, and subsequently used thosestates to model each student’s design approach. Interestingly, students in the two experimentalconditions used markedly different approaches. In this way, then, the analysis served to reveal some ofthe behavioural differences associated with the two different experimental conditions. The analysis alsorevealed that the multimodal behaviours observed had clear correlations with prior work onepistemological framing (Russ, Lee, & Sherin, 2012).(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 233The strategies used in Worsley (2014) and Worsley and Blikstein (2014) represent a small fraction of thework being done in the multimodal analysis community, which spans a variety of complex approachesfor doing multimodal fusion at different levels of analysis, as well as using a variety of algorithms, datarepresentations, and strategies for training and testing said algorithms (see Song, Morency, & Davis,2012; Scherer et al., 2012; and Ngiam et al., 2011 for more details.) A particular challenge, however, isreconciling the complexities of these computational approaches with actionable ideas and theories forlearning.Taken together, the prior research points to a wealth of technology and methodologies that can be usedfor doing multimodal analysis of student learning across a diversity of environments. By studyinglearning through these different lenses we can better identify how students are changing, and makemore sense of their changes. Furthermore, multimodal analysis enables researchers to get far morenuanced and complex understandings of student learning processes, something that we have onlybegun to study at scale.3 CONCLUSIONIn this article, we have presented a review of the literature on what we have termed “multimodallearning analytics” — a set of techniques employing multiple sources of data (video, logs, text, artifacts,audio, gestures, biosensors) to examine learning in realistic, ecologically valid, social, mixed-medialearning environments.The incorporation of multimodal techniques, which are extensively used in the multimodal interactioncommunity, should enable researchers to examine unscripted, complex tasks in more holistic ways. Inparticular, we have focused on describing a set of modalities that have been the topic of multimodalanalysis for decades, as well as modalities that have recently emerged as new data streams throughwhich researchers can study human interaction and behaviour.REFERENCESAlvarado, C., & Davis, R. (2006). Dynamically constructed Bayes nets for multi-domain sketchunderstanding. Proceedings of the ACM SIGGRAPH 2006 Courses (A. 33).http://dx.doi.org/10.1145/1281500.1281544Alvarado, C., Oltmans, M., & Davis, R. (2002). A framework for multi-domain sketch recognition.Proceedings of AAAI Spring Symposium on Sketch Understanding, Palo Alto, California (pp. 1–8).Retrieved from http://rationale.csail.mit.edu/publications/Alvarado2002Framework.pdfAnthony, L., Yang, J., Koedinger, K. (2007). Adapting handwriting recognition for applications in algebralearning. Proceedings of the ACM Workshop on Educational Multimedia and MultimediaEducation (EMME 2007), 47–56. http://dx.doi.org/10.1145/1290144.1290153(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 234Baker, R., D’Mello, S. K., Rodrigo, M. M. T., Graesser, A. C. (2010). Better to be frustrated than bored:The incidence, persistence, and impact of learners’ cognitive-affective states during interactionswith three different computer-based learning environments. International Journal of Human–Computer Studies, 68(4), 223–241. http://dx.doi.org/10.1016/j.ijhcs.2009.12.003Barron, B., & Darling-Hammond, L. (2010). Prospects and challenges for inquiry-based approaches tolearning. In H. Dumont, D. Istance, & F. Benavides (Eds.), The nature of learning: Using researchto inspire practice (pp. 199-225). Paris: OECD.Beck, J. E., & Sison, J. (2006). Using knowledge tracing in a noisy environment to measure studentreading proficiencies. International Journal of Artificial Intelligence in Education, 16(2), 129–143.Burt, K. B., & Obradović, J. (2013). The construct of psychophysiological reactivity: Statistical andpsychometric issues. Developmental Review, 33(1), 29–57.http://dx.doi.org/10.1016/j.dr.2012.10.002Chang, K. M., Nelson, J., Pant, U., & Mostow, J. (2013). Toward exploiting EEG input in a readingtutor. International Journal of Artificial Intelligence in Education, 22(1), 19–38.http://dx.doi.org/10.1007/978-3-642-21869-9_31Chang, M., & Forbus, K. (2012). Using quantitative information to improve analogical matching betweensketches. In Proceedings of the 24th Annual Conference on Innovative Applications of ArtificialIntelligence (AAAI-12), Toronto, Canada, 2269–2274.Choi, M. H. C., Su-Jeong Lee, S. J. L., Jae-Woong Yang, J. W. Y., Ji-Hye Kim, J. H. K., Jin-Seung Choi, J. S. C.,Jang-Yeon Park, J. Y. P., ... & Dae-Woon Lim, D. W. L. (2010). Changes in cognitive performancedue to three types of emotional tension. International Journal of Bio-Science and Bio-Technology, 2(4), 23–28. http://dx.doi.org/10.1007/978-3-642-17622-7_26Conati, C., & Maclaren, H. (2009). Empirically building and evaluating a probabilistic model of user affect.User Modeling and User-Adapted Interaction, 19(3), 267–303.http://dx.doi.org/10.1007/s11257-009-9062-8Cowley, B., Ravaja, N., & Heikura, T. (2013). Cardiovascular physiology predicts learning effects in aserious game activity. Computers & Education, 60(1), 299–309.http://dx.doi.org/10.1016/j.compedu.2012.07.014Craig, S. D., D’Mello, S., Witherspoon, A., & Graesser, A. (2008). Emote aloud during learning withAutoTutor: Applying the facial action coding system to cognitive-affective states during learning.Cognition & Emotion, 22(5), 777–788. http://dx.doi.org/10.1080/02699930701516759D’Mello, S., Craig, S., Witherspoon, A., McDaniel, B., & Graesser, A. (2008). Automatic detection oflearner’s affect from conversational cues. User Modeling and User-Adapted Interaction, 18(1),45–80.http://dx.doi.org/10.1007/s11257-007-9037-6de Koning, B. B., Tabbers, H. K., Rikers, R. M., & Paas, F. (2010). Attention guidance in learning from acomplex animation: Seeing is understanding? Learning and Instruction, 20(2), 111–122.http://dx.doi.org/10.1016/j.learninstruc.2009.02.010Dewey, J. (1902). The school and society. Chicago, Il: University of Chicago Press.(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 235Dym, C. L. (1999). Learning engineering: Design, languages, and experiences. Journal of EngineeringEducation, 88(2), 145–148. http://dx.doi.org/10.1002/j.2168-9830.1999.tb00425.xForbes-Riley, K., Rotaru, M., & Litman, J. (2009). The relative impact of student affect on performancemodels in a spoken dialogue tutoring system. User Modeling and User-Adapted Interaction(Special Issue on Affective Modeling and Adaptation), 18(1–2), 11–43.http://dx.doi.org/10.1007/s11257-007-9038-5Forbus, K., Usher, J., Lovett, A., Lockwood, K., & Wetzel, J. (2011). CogSketch: Sketch understanding forcognitive science research and for education. Topics in Cognitive Science, 3(4), 648–666.http://dx.doi.org/10.1111/j.1756-8765.2011.01149.xFreire, P. (1970). Pedagogia do Oprimido. Rio de Janeiro: Paz e Terra.Gomes, J. S., Yassine, M., Worsley, M., & Blikstein, P. (2013). Analysing engineering expertise of highschool students using eye tracking and multimodal learning analytics. In S. K. DʼMello, R. A.Calvo, & A. Olney (Eds.), Proceedings of the 6th International Conference on Educational DataMining (EDM 2013), (pp. 375–377). Retrieved fromhttp://www.educationaldatamining.org/EDM2013/papers/rn_paper_88.pdfHowison, M., Trninic, D., Reinholz, D., & Abrahamson, D. (2011). The Mathematical Imagery Trainer:from embodied interaction to conceptual learning. In G. Fitzpatrick, C. Gutwin, B. Begole, W. A.Kellogg & D. Tan (Eds.), Proceedings of the annual meeting of CHI: ACM Conference on HumanFactors in Computing Systems (CHI 2011), (Vol. \"Full Papers\", pp. 1989-1998).http://dx.doi.org/10.1145/1978942.1979230Hussain, M. S., AlZoubi, O., Calvo, R. A., & D’Mello, S. K. (2011). Affect detection from multichannelphysiology during learning sessions with AutoTutor. In G. Biswas, S. Bull, J. Kay, A. Mitrovic(Eds.), Proceedings of the 15th International Conference on Artificial Intelligence in Education(AIED), (pp. 131–138). http://dx.doi.org/10.1007/978-3-642-21869-9_19Jee, B., Gentner, D., Forbus, K., Sageman, B., & Uttal, D. (2009). Drawing on experience: Use of sketchingto evaluate knowledge of spatial scientific concepts. In N. Taatgen & H. van Rijn (Eds.),Proceedings of the 31st Annual Conference of the Cognitive Science Society (CogSci 2009), (pp.2499–2504). Retrieved from http://blog.silccenter.org/publications_pdfs/jee etal_CogSci2009_Final.pdfJermann, P., Mullins, D., Nüssli, M.-A., & Dillenbourg, P. (2011). Collaborative gaze footprints: Correlatesof interaction quality. In H. Spada, G. Stahl, N. Miyake, N. Law (Eds.), Proceedings of the 11thInternational Conference on Computer-Supported Collaborative Learning (CSCL 2011), (Vol.1, pp.184–191). Hong Kong: International Society of the Learning Sciences.Kirschner, P. A., Sweller, J., & Clark, R. E. (2006). Why minimal guidance during instruction does notwork: An analysis of the failure of constructivist, discovery, problem-based, experiential, andinquiry-based teaching. Educational Psychologist, 41(2), 75–86.http://dx.doi.org/10.1207/s15326985ep4102_1Klahr, D., & Nigam, M. (2004). The equivalence of learning paths in early science instruction.Psychological Science, 15(10), 661. http://dx.doi.org/10.1111/j.0956-7976.2004.00737.x(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 236Levy, F., & Murnane, R. J. (2004). The new division of labor: How computers are creating the next jobmarket. Princeton, NJ: Princeton University Press.Litman, D., Moore, J., Dzikovska, M., & Farrow, E. (2009). Using natural language processing to analyzetutorial dialogue corpora across domains and modalities. Proceedings of the 2009 conference onArtificial Intelligence in Education: Building Learning Systems that Care: From KnowledgeRepresentation to Affective Modelling (AIED ʼ09), (pp. 149–156). The Netherlands: IOS Press.Retrieved from http://people.cs.pitt.edu/~litman/paper_125.pdfLuft, C. D., Nolte, G., & Bhattacharya, J. (2013). High-learners present larger mid-frontal theta power andconnectivity in response to incorrect performance feedback. The Journal of Neuroscience, 33(5),2029–2038. http://dx.doi.org/10.1523/JNEUROSCI.2565-12.2013Lunn, D., & Harper, S. (2010). Using galvanic skin response measures to identify areas of frustration forolder web 2.0 users. Proceedings of the 2010 International Cross Disciplinary Conference on WebAccessibility (W4A ’10), (p. 34). http://dx.doi.org/10.1145/1805986.1806032Mason, L., Pluchino, P., Tornatora, M. C., & Ariasi, N. (2013). An eye-tracking study of learning fromscience text with concrete and abstract illustrations. The Journal of Experimental Education,81(3), 356–384.http://dx.doi.org/10.1080/00220973.2012.727885Montessori, M. (1965). Spontaneous activity in education. New York: Schocken Books.Muldner, K., Burleson, W., & VanLehn, K. (2010). “Yes!” Using tutor and sensor data to predict momentsof delight during instructional activities. In P. de Bra, A. Kobsa, D. Chin (Eds.), Proceedings of18th International Conference, UMAP 2010: User modeling, adaptation, and personalization(Lecture Notes in Computer Science Series), (Vol. 6075, pp. 159–170).http://dx.doi.org/10.1007/978-3-642-13470-8_16Munteanu, C., Penn, G., & Zhu, X. (2009). Improving automatic speech recognition for lectures throughtransformation-based rules learned from minimal data. Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP (Vol. 2, Vol. 2), (pp.764–772). Stroudsburg, PA: ACM.Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., & Ng, A. Y. (2011). Multimodal deep learning. Proceedingsof the 28th International Conference on Machine Learning (ICML-11), (pp. 689–696). Retrieved from http://machinelearning.wustl.edu/mlpapers/papers/ICML2011Ngiam_399Ocumpaugh, J., Baker, R. S. J. d., Rodrigo, M. M. T. (2012). Baker-Rodrigo Observation Method Protocol(BROMP) 1.0: Training Manual. Version 1.0. [Technical Report]. New York: EdLab. Manila,Philippines: Ateneo Laboratory for the Learning Sciences.Papert, S. (1980). Mindstorms: Children, computers, and powerful ideas. New York: Basic Books.Pardos, Z. A., Baker, .R. S. J. d., San Pedro, M. O. C. Z., Gowda, S. M., Gowda, S. M. (2013). Affectivestates and state tests: Investigating how affect throughout the school year predicts end of yearlearning outcomes. Proceedings of the 3rd International Conference on Learning Analytics andKnowledge (LAK ’13), 117–124. http://dx.doi.org/10.1145/2460296.2460320(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 237Raca, M., Tormey, R., & Dillenbourg, P. (2014). Sleepers’ lag-study on motion and attention. Proceedingsof the 4th International Conference on Learning Analytics and Knowledge (LAK ʼ14), 36–43.http://dx.doi.org/10.1145/2567574.2567581Read, J. C. (2007). A study of the usability of handwriting recognition for text entry by children.Interacting with Computers, 19(1), 57–69. http://dx.doi.org/10.1007/1-84628-062-1_9Richardson, D. C., & Dale, R. (2005). Looking to understand: The coupling between speakers’ andlisteners’ eye movements and its relationship to discourse comprehension. Cognitive science,29(6), 1045–1060. http://dx.doi.org/10.1207/s15516709cog0000_29Russ, R. S., Lee, V. R., & Sherin, B. L. (2012). Framing in cognitive clinical interviews about intuitivescience knowledge: Dynamic student understandings of the discourse interaction. ScienceEducation, 96(4), 573–599. http://dx.doi.org/10.1002/sce.21014Scherer, S., Glodek, M., Layher, G., Schels, M., Schmidt, M., Brosch, T., ... & Palm, G. (2012). A genericframework for the inference of user states in human computer interaction. Journal onMultimodal User Interfaces, 6(3–4), 117–141.Schick, A. Morlock, D., Amma, C., Schultz, T., & Stiefelhagen, R. (2012). Vision-based handwritingrecognition for unrestricted text input in mid-air. Proceedings of the 14th ACM InternationalConference on Multimodal Interaction (ICMI ’12), 217–220. New York: ACM.http://dx.doi.org/10.1145/2388676.2388719Schlömer, T., Poppinga, B., Henze, N., & Boll, S. (2008). Gesture recognition with a Wii controller.Proceedings of the 2nd International Conference on Tangible and Embedded Interaction (TEI ’08),11–14. http://dx.doi.org/10.1145/1347390.1347395Schneider, B., & Blikstein, P. (2014). Unraveling Students’ Interaction Around a Tangible Interface usingGesture Recognition. In J. Stamper, Z. Pardos, M. Mavrikis, B. Mclauren (Eds.), Proceedings ofthe 7th International Conference on Educational Data Mining (EDM’14), (pp.320-323). Retrievedfrom http://educationaldatamining.org/EDM2014/proceedings/EDM2014Proceedings.pdfSchneider, B., & Pea, R. (2013). Real-time mutual gaze perception enhances collaborative learning andcollaboration quality. International Journal of Computer-Supported Collaborative Learning, 8(4),375–397. http://dx.doi.org/10.1007/s11412-013-9181-4Schneider, B., & Pea, R. (2014). Toward Collaboration Sensing. International Journal of Computer-Supported Collaborative learning, 9(4), 371-395. http://dx.doi.org/10.1007/s11412-014-9202-ySchneider, B., Sharma, K., Cuendet, S., Zufferey, G., Dillenbourg, P., & Pea, A. D. (2015). 3D tangiblesfacilitate joint visual attention in dyads. In O. Lindwall, P. Hakkinen, T. Koschmann, P.Tschounikine, S. Ludvigsen (Eds.), In Proceedings of the International Conference on ComputerSupported Collaborative Learning 2015: Exploring the Material Conditions of Learning (CSCL’15),(Vol.1, pp. 158–165). Gothenburg, Sweden: The International Society of the Learning Sciences.She, H. C., Jung, T. P., Chou, W. C., Huang, L. Y., Wang, C. Y., & Lin, G. Y. (2012). EEG dynamics reflect thedistinct cognitive process of optic problem solving. PLOS ONE, 7(7), e40731.http://dx.doi.org/10.1371/journal.pone.0040731(2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journalof Learning Analytics, 3(2), 220–238. http://dx.doi.org/10.18608/jla.2016.32.11ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) 238Sherin, B. (2013). A computational study of commonsense science: An exploration in the automatedanalysis of clinical interview data. Journal of the Learning Sciences, 22(4), 600–638.http://dx.doi.org/10.1080/10508406.2013.836654Song, Y., Morency, L. P., & Davis, R. (2012). Multimodal human behavior analysis: Learning correlationand interaction across modalities. Proceedings of the 14th ACM International Conference onMultimodal Interaction (ICMI ’12), 27–30. http://dx.doi.org/10.1145/2388676.2388684Stevens, R. H., Galloway, T., & Berka, C. (2007). EEG-related changes in cognitive workload, engagementand distraction as students acquire problem solving skills. In C. Conati, K. McCoy, G. Paliouras(Eds.), Proceedings of the 11th International Conference on User Modeling, (pp. 187–196). BerlinHeidelberg: Springer Link. http://dx.doi.org/10.1007/978-3-540-73078-1_22Weinland, D., Ronfard, R., & Boyer, E. (2006). Free viewpoint action recognition using motion historyvolumes. Computer Vision and Image Understanding, 104(2), 249–257.http://dx.doi.org/10.1016/j.cviu.2006.07.013Worsley, M. (2014). Multimodal learning analytics as a tool for bridging learning theory and complexlearning behaviors. Proceedings of the 2014 ACM workshop on Multimodal Learning AnalyticsWorkshop and Grand Challenge (MLA ’14), 1–4. http://dx.doi.org/10.1145/2666633.2666634Worsley, M., & Blikstein, P. (2011). What’s an expert? Using learning analytics to identify emergentmarkers of expertise through automated speech, sentiment and sketch analysis. In M.Pechenizkiy, T. Calders, C. Conati, S. Ventura, C. Romero, & J. Stamper (Eds.), Proceedings of the4th Annual Conference on Educational Data Mining (EDM 2011), (pp. 235–240).Worsley, M., & Blikstein, P. (2013). Toward the development of multimodal action based assessment.Proceedings of the 3rd International Conference on Learning Analytics and Knowledge (LAK ’13),94–101. http://dx.doi.org/10.1145/2460296.2460315Worsley, M., & Blikstein, P. (2014). Deciphering the practices and affordances of different reasoningstrategies through multimodal learning analytics. Proceedings of the 2014 ACM workshop onMultimodal Learning Analytics Workshop and Grand Challenge (MLA ’14), 21–27.http://dx.doi.org/10.1145/2666633.2666637Worsley, M., & Blikstein, P. (2015). Using learning analytics to study cognitive disequilibrium in acomplex learning environment. Proceedings of the 5th International Conference on LearningAnalytics and Knowledge (LAK ʼ15), 426–427. http://dx.doi.org/10.1145/2723576.2723659Yilmaz, A., & Shah, M. (2005). Actions sketch: A novel action representation. In C. Schmid, S. Soatto, C.Tomasi (Eds.), Proceedings of the 2005 IEEE Computer Society Conference on Computer Visionand Pattern Recognition (CVPR 2005), (Vol. 1, pp. 984–989). Los Alamitos, CA: IEEE ComputerSociety. http://dx.doi.org/10.1109/CVPR.2005.58Zaïane, O. R. (2001). Web usage mining for a better web-based learning environment. In Proceedings ofthe Conference on Computers and Advanced Technology in Education (CATE 2001), (pp. 60–64).https://webdocs.cs.ualberta.ca/~zaiane/postscript/CATE2001.pdf"

  const splitter = new TokenTextSplitter({
    encodingName: 'gpt2',
    chunkSize: 400,
    chunkOverlap: 0,
  })
  console.log("chunkSize")
  console.log(fileText)
  const returnList = []
  let res = await splitter.splitText(fileText)
  console.log("initial res ↓")
  console.log(res)
  while (res.length > 1) {
    returnList.push(...res.slice(0, res.length - 1))
    res = await splitter.splitText(res[res.length - 1])
  }
  returnList.push(...res)
  console.log(returnList)
  return returnList
}
// test()
registerPortListener(async (session, port, config) => await executeApi(session, port, config))
// registerCommands()
registerImageSaver()
refreshMenu()
// await registerTextSpliter()
